{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38eb386d-aaa1-4b89-bbcd-9d3f05b3628f",
   "metadata": {},
   "source": [
    "# LLMs prompts\n",
    "\n",
    "School of Epidemiology\n",
    "\n",
    "Futuri Center, Białystok, October 22, 2025 \n",
    "\n",
    "Wojciech Łaguna\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54165eaf-17a8-4c6c-bae5-ec5eac11aa8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <span style=\"color: red;\">Warning!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3277b3-dd10-4196-ad5e-096a7c2ecb7d",
   "metadata": {},
   "source": [
    "The following materials are for data review purposes only. The results of the prompts **should not be used in scientific work under any circumstances**. The prompts should only be used for preliminary data review. Final statistics and data analysis should be performed by a professional statistician. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7bff00-cf7d-4d52-ae0e-aa2202c8913c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d09980-c38e-4b58-ae92-aedfb2d63adf",
   "metadata": {},
   "source": [
    "1. [Anaconda](https://www.anaconda.com/download/success) (preferred) or stanalone [Jupyter Notebook](https://jupyter.org/install) installed\n",
    "\n",
    "![image](../Images/anaconda_download.png)\n",
    "\n",
    "2. Jupyter Lab selected (right-up corner of the image)\n",
    "\n",
    "![image](../Images/jupyter_lab.png)\n",
    "   \n",
    "3. Access to **one of the following LLMs** (free or paid version). In some you need to be registered:\n",
    "   - [Claude](https://claude.ai/login?returnTo=%2F%3F)\n",
    "   - [ChatGPT](https://chatgpt.com/)\n",
    "   - [Gemini](https://gemini.google.com/)\n",
    "   - [GROK](https://grok.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbbc2e3-584b-4354-b086-9ec7d51a8fa8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292d19d6-d660-4351-b458-53e6d0574e9e",
   "metadata": {},
   "source": [
    "**[Heart Disease Cleveland](https://www.kaggle.com/datasets/ritwikb3/heart-disease-cleveland/data)**\n",
    "\n",
    "The dataset is the Cleveland Heart Disease dataset taken from the UCI repository. The dataset consists of 303 individuals’ data.\n",
    "\n",
    "\n",
    "[Licence:CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1e0418-e2b2-4f07-aeae-afb6940fa656",
   "metadata": {},
   "source": [
    "1. **Age**: Patients Age in years \n",
    "2. **Sex**: Gender (Male : 1; Female : 0) \n",
    "3. **cp**: Type of chest pain experienced by patient. This term categorized into 4 category.\n",
    "    - 0 typical angina,\n",
    "    - 1 atypical angina,\n",
    "    - 2 non-anginal pain,\n",
    "    - 3 asymptomatic \n",
    "4. **trestbps**: patient's level of blood pressure at resting mode in mm/HG \n",
    "5. **chol**: Serum cholesterol in mg/dl \n",
    "6. **fbs**: Blood sugar levels on fasting > 120 mg/dl represents as 1 in case of true and 0 as false \n",
    "7. **restecg**: Result of electrocardiogram while at rest are represented in 3 distinct values\n",
    "   - 0 : Normal\n",
    "   - 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "   - 2: showing probable or definite left ventricular hypertrophyby Estes' criteria \n",
    "8. **thalach**: Maximum heart rate achieved \n",
    "9. **exang**: Angina induced by exercise 0 depicting NO 1 depicting Yes\n",
    "10. **oldpeak**: Exercise induced ST-depression in relative with the state of rest \n",
    "11. **slope**: ST segment measured in terms of slope during peak exercise\n",
    "    - 0: up sloping;\n",
    "    - 1: flat;\n",
    "    - 2: down sloping\n",
    "14. **ca**: The number of major vessels (0–3)\n",
    "15. **thal**: A blood disorder called thalassemia\n",
    "    - 0: NULL\n",
    "    - 1: normal blood flow\n",
    "    - 2: fixed defect (no blood flow in some part of the heart)\n",
    "    - 3: reversible defect (a blood flow is observed but it is not normal\n",
    "16. ***target***: It is the target variable which we have to predict. 1 means patient is suffering from heart disease and 0 means patient is normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a96abc8-033e-41e9-ae6d-6c4cd9dc7d57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16acf348-cd4c-4ace-ab3e-f66ef4cddc0d",
   "metadata": {},
   "source": [
    "**PROMPT: I need to load a dataset from given address to pandas 'https://raw.githubusercontent.com/wojla/llm_workshop/refs/heads/main/data/Heart_disease_cleveland_new.csv'. Plase assign it to \"df\" variable**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e6acd-ed6b-44bd-89a7-9c8b63cc9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the given path\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/wojla/llm_workshop/refs/heads/main/data/Heart_disease_cleveland_new.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd7f2f3-3fd1-4d58-8a2f-f35aebf345be",
   "metadata": {},
   "source": [
    "**PROMPT: Can I see table in a nicer outline?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d50cb-3ee9-4467-b510-5e7e478d8377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b59edf7-6949-4e69-be23-bc10f6374dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.read_csv('../data/Heart_disease_cleveland_new.csv')\n",
    "\n",
    "# Pretty table print in terminal\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1553eea-fe0d-469f-b85a-867d2b7b07c1",
   "metadata": {},
   "source": [
    "**PROMPT: What types of data do I have in my dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352babe-ca5e-4750-aacf-600335b946f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic info about data types\n",
    "display(df.dtypes.to_frame(name='Data Type').rename_axis('Column Name').reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0cc3f-470f-4850-af99-ba6bb3b45762",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ac33b-4788-48d0-ac05-9db156b45cd0",
   "metadata": {},
   "source": [
    "**PROMPT: I know that \"chol\" should be a number, not \"object.\" How can I check if this and other columns displaying the \"object\" type contain a random string?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61feb02f-5ea6-4675-ad5f-cc165e06016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over columns with type 'object'\n",
    "invalid_entries = {}\n",
    "\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    # Try converting to numeric, track invalid (non-convertible) entries\n",
    "    mask = pd.to_numeric(df[col], errors='coerce').isna() & df[col].notna()\n",
    "    if mask.any():\n",
    "        invalid_entries[col] = df.loc[mask, col].unique()\n",
    "\n",
    "# Display results\n",
    "if invalid_entries:\n",
    "    display(pd.DataFrame(\n",
    "        [(k, ', '.join(map(str, v))) for k, v in invalid_entries.items()],\n",
    "        columns=['Column', 'Invalid Entries']\n",
    "    ))\n",
    "else:\n",
    "    print(\"✅ No non-numeric values found in object-type columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1cce54-d87d-465f-b8ee-f22fb4ad8538",
   "metadata": {},
   "source": [
    "**PROMPT: How can I replace \"unknown\" by an empty value?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa6a99-2b25-437b-9605-e13f0d96be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df.replace(\"unknown\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c8c0d-8e2d-4411-a4cb-46c919bf0010",
   "metadata": {},
   "source": [
    "**PROMPT: I replaced, but \"chol\" is still an \"object\" in my dataframe.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661bd50e-bfe1-486d-89d5-9f6e1378c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['chol'] = pd.to_numeric(df['chol'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee47ec51-f6b4-4ee1-b26c-5ad9e7b9fc6d",
   "metadata": {},
   "source": [
    "**PROPMPT: How to discover outliers in my dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d648e-4c1c-46c6-9e01-8bb5f021b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR and detect outliers for each numeric column\n",
    "outlier_summary = []\n",
    "for col in df.select_dtypes(include=['number']).columns:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "    outlier_summary.append((col, len(outliers), outliers.min(), outliers.max()))\n",
    "\n",
    "display(pd.DataFrame(outlier_summary, columns=['Column', 'Outlier Count', 'Min Outlier', 'Max Outlier']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4711e-2f5a-4055-872f-d25efedc4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.select_dtypes(include=['number']).boxplot(figsize=(12,6))\n",
    "plt.title('Boxplots for Numeric Features (IQR Outlier Detection)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1d97b-4843-41f9-b164-973addb60b56",
   "metadata": {},
   "source": [
    "**PROMPT: I see that the maximum values ​​for \"chol\" and \"thalach\" are outliers, as are the minimum values ​​for \"target.\" How do I replace them with an empty value?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0dd3f-4093-4660-a0e6-231a1c55a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 'chol' — replace the maximum value with NaN\n",
    "df.loc[df['chol'] == df['chol'].max(), 'chol'] = np.nan\n",
    "\n",
    "# For 'thalach' — replace the maximum value with NaN\n",
    "df.loc[df['thalach'] == df['thalach'].max(), 'thalach'] = np.nan\n",
    "\n",
    "# For 'target' — replace the minimum value with NaN\n",
    "df.loc[df['target'] == df['target'].min(), 'target'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bc0ba3-dd4d-4193-9ea4-da4e2c3eddac",
   "metadata": {},
   "source": [
    "**PROMPT: How many missing values are in my dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47186e89-a272-433c-92f9-5f1529874039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values per column\n",
    "missing_summary = (\n",
    "    df.isna().sum()\n",
    "    .to_frame(name='Missing Values')\n",
    "    .rename_axis('Column Name')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Display as a nice table\n",
    "display(missing_summary)\n",
    "\n",
    "# Optionally, total number of missing values in the dataset\n",
    "print(f\"\\nTotal missing values in dataset: {df.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cecc8b-32e7-4aab-85f5-931dbfd926ac",
   "metadata": {},
   "source": [
    "**PROMPT: How to remove all records that contain at least one empty value?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d0558-20c9-4e4d-a8db-05c0f4dc07bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "print((df.isna().any(axis=1)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf2389-06e8-41b4-9539-3d765c284b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad66f8-824f-45dd-80e0-e91aee44ab20",
   "metadata": {},
   "source": [
    "**PROMPT: How to check if there any duplicates?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a3b9c8-7ffc-4900-bb70-18cc89917c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many duplicate rows exist\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c83dc9f-e277-4b15-8863-7a6f62eff772",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdbca72-afa9-4c93-9de2-40d0241d3e91",
   "metadata": {},
   "source": [
    "**PROMPT: How to obtain column names containing continuous and categorical values?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b9c0a8-eeea-4104-8a6d-96c28d8b0e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "# Separate continuous vs discrete numeric (optional refinement)\n",
    "# Continuous: many unique numeric values\n",
    "# Discrete: few unique numeric values (e.g., 0/1 target or small category codes)\n",
    "continuous_cols = [col for col in numeric_cols if df[col].nunique() > 10]\n",
    "discrete_cols = [col for col in numeric_cols if df[col].nunique() <= 10]\n",
    "\n",
    "# Display results\n",
    "display(pd.DataFrame({\n",
    "    'Continuous Columns': pd.Series(continuous_cols),\n",
    "    'Discrete/Categorical Columns': pd.Series(categorical_cols + discrete_cols)\n",
    "}))\n",
    "print(\"continuous_cols\": continuous_cols)\n",
    "print(\"discrete_cols\": discrete_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d752ff1-a7e2-4a42-a189-62b4e695c33b",
   "metadata": {},
   "source": [
    "**PROMPT: How to display histograms for continuous data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58110b-1c26-4019-8829-17c332b2e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Identify continuous columns (if not already done)\n",
    "continuous_cols = [col for col in df.select_dtypes(include='number').columns if df[col].nunique() > 10]\n",
    "\n",
    "# Plot histograms\n",
    "for col in continuous_cols:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(df[col].dropna(), bins=30)  # drop NaNs just in case\n",
    "    plt.title(f'Histogram of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c7c1d-5313-4801-b8b5-5eee9bf40635",
   "metadata": {},
   "source": [
    "**PROMPT: How to get basic statistic for continuous data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba5831d-bef2-4f12-a86b-ccc5e14e46c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic statistics for continuous data\n",
    "stats = df[continuous_cols].describe().T  # transpose for easier reading\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bed3c1-2e3c-42b1-8c86-605834ecb22b",
   "metadata": {},
   "source": [
    "**PROMPT: How to visualize columns with categorical values?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9348853d-9035-4228-b6b0-b0211d3e6523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify discrete/categorical columns\n",
    "categorical_cols = [col for col in df.columns if df[col].dtype == 'object' or df[col].nunique() <= 10]\n",
    "\n",
    "# Plot bar charts\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    df[col].value_counts().plot(kind='bar')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75948bd8-32e9-4daf-936f-02379f0093e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts().plot(kind='pie', autopct='%1.1f%%', figsize=(5,5))\n",
    "plt.title('Target distribution')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0dfcb-f697-4f46-8c42-9791daa8a920",
   "metadata": {},
   "source": [
    "**PROMPT: How to get basic statistics for categorical variables?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26782a9-9f8f-4436-a3a7-8917d9f2c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [col for col in df.columns if df[col].dtype == 'object' or df[col].nunique() <= 10]\n",
    "p_value\n",
    "# Compute basic stats for each categorical column\n",
    "cat_stats = pd.DataFrame({\n",
    "    'Unique Values': [df[col].nunique() for col in categorical_cols],\n",
    "    'Most Frequent': [df[col].mode()[0] if not df[col].mode().empty else None for col in categorical_cols],\n",
    "    'Frequency of Most Frequent': [df[col].value_counts().iloc[0] if not df[col].value_counts().empty else 0 for col in categorical_cols],\n",
    "    'Missing Values': [df[col].isna().sum() for col in categorical_cols]\n",
    "}, index=categorical_cols)\n",
    "\n",
    "display(cat_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7d5d1-2791-45c4-83ea-5c15b7710b6b",
   "metadata": {},
   "source": [
    "**PROMPT: Can I get a table showing the numbers and percentages for each categorical variable?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24ac49-2b06-4f9e-b51c-1d0787afdb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a summary table\n",
    "cat_summary = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    counts = df[col].value_counts(dropna=False)\n",
    "    percentages = (counts / len(df) * 100).round(2)\n",
    "    temp_df = pd.DataFrame({'Category': counts.index, 'Count': counts.values, 'Percentage': percentages.values})\n",
    "    temp_df.insert(0, 'Variable', col)\n",
    "    cat_summary.append(temp_df)\n",
    "\n",
    "# Combine all summaries into one table\n",
    "cat_summary_df = pd.concat(cat_summary, ignore_index=True)\n",
    "\n",
    "display(cat_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42777ec-4b9d-49ce-ac06-b5f1ebac9fcc",
   "metadata": {},
   "source": [
    "**PROMPT: Can I see correlation matrix for my data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fbd3fc-7e9e-4274-983b-4ea8e9797963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation for numeric columns only\n",
    "corr_matrix = df.select_dtypes(include='number').corr()\n",
    "\n",
    "# Display neatly\n",
    "display(corr_matrix.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22624a-f8a5-4261-a783-b2ae4f6b21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.select_dtypes(include='number').corr()\n",
    "\n",
    "# Plot heatmap with values\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar(label='Correlation')\n",
    "\n",
    "# Axis ticks\n",
    "plt.xticks(ticks=np.arange(len(corr_matrix.columns)), labels=corr_matrix.columns, rotation=90)\n",
    "plt.yticks(ticks=np.arange(len(corr_matrix.columns)), labels=corr_matrix.columns)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "\n",
    "# Add text annotations for correlation values\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(len(corr_matrix.columns)):\n",
    "        plt.text(j, i, f\"{corr_matrix.iloc[i, j]:.1f}\",\n",
    "                 ha='center', va='center', color='black', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684cfcc8-692a-458a-a98e-cf04810c4a50",
   "metadata": {},
   "source": [
    "**PROMPT: I do not see \"sex\" in the list (Male, Female), how to include them?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a53e7-7add-4627-ab32-ddad76628704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numeric version for correlation\n",
    "df['sex_num'] = df['sex'].map({'Female': 0, 'Male': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d5414-4b20-4c70-8d09-91e14c577076",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Statistics on groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13de8bfc-2046-4127-aec0-f198be15a47f",
   "metadata": {},
   "source": [
    "**PROMPT: Can you show basic statistics per \"sex\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4472fab1-5c27-4de3-bfc8-84a474d9445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics grouped by 'sex'\n",
    "sex_stats = df.groupby('sex').describe().T  # Transpose for readability\n",
    "display(sex_stats)\n",
    "display(df.groupby('sex').agg(['mean', 'std', 'min', 'max']).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5430f712-e70c-4818-8cc8-cfd23903cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "p_value\n",
    "df.boxplot(column='chol', by='sex', figsize=(6,4))\n",
    "plt.title('Cholesterol Levels by Sex')\n",
    "plt.suptitle('')\n",
    "plt.xlabel('Sex')\n",
    "plt.ylabel('Cholesterol')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2626461d-58f3-46aa-9a06-f9b00866462b",
   "metadata": {},
   "source": [
    "**PROMPT: Can you show how many Men have \"chol\" level above the median for whole dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af9234-d0c4-46fb-90ec-07c9e031c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset-wide median\n",
    "chol_median = df['chol'].median()\n",
    "\n",
    "# Boolean mask for men\n",
    "men_mask = df['sex'].eq('Male')\n",
    "\n",
    "# Count men with chol > median (NaNs are excluded)\n",
    "men_above_median = (men_mask & df['chol'].notna() & df['chol'].gt(chol_median)).sum()\n",
    "\n",
    "print(f\"Median chol (entire dataset): {chol_median:.2f}\")\n",
    "print(f\"Men with chol > median: {int(men_above_median)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b0cb4-6a27-40f7-aaf4-12555c69e5f8",
   "metadata": {},
   "source": [
    "**PROMPT: Show me records with Man who has \"chol\" levels above 200**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087535f4-48a9-4ec4-b748-ec05496806c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['sex'].eq('Male') & (pd.to_numeric(df['chol'], errors='coerce') > 200)\n",
    "\n",
    "men_high_chol = df.loc[mask].copy()\n",
    "\n",
    "men_high_chol = men_high_chol.sort_values('chol', ascending=False)\n",
    "men_high_chol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead3130-3a45-47e3-99bc-f634c745ea57",
   "metadata": {},
   "source": [
    "**PROMPT: How to save selected Men to a csv file?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca5e0e-6dd7-4403-8db5-584df5f2b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"men_high_chol.csv\"  # change path if needed\n",
    "men_high_chol.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"Saved {len(men_high_chol)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2cb07c-fee2-455a-9e1c-6aea48eccf16",
   "metadata": {},
   "source": [
    "**PROMPT: Can you show a plot of \"chol\" vs \"age\" regarding \"target\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62fb05-c66b-418a-a504-7ee34db63ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# Scatter plot: color by target (e.g., 0 = no disease, 1 = disease)\n",
    "for t in df['target'].unique():\n",
    "    subset = df[df['target'] == t]\n",
    "    plt.scatter(subset['age'], subset['chol'], label=f'Target {t}', alpha=0.7)\n",
    "\n",
    "plt.title('Cholesterol vs Age by Target')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Cholesterol (chol)')\n",
    "plt.legend(title='Target')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9004dfa-81f1-4ee4-927c-ac3140b2f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Define target labels for readability\n",
    "target_labels = {0: 'No Disease', 1: 'Disease'}\n",
    "\n",
    "# Compute and plot regression for each target group\n",
    "for t, label in target_labels.items():\n",
    "    subset = df[df['target'] == t]\n",
    "    x = subset['age']\n",
    "    y = subset['chol']\n",
    "\n",
    "    # Perform linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "\n",
    "    # Scatter plot for this group\n",
    "    plt.scatter(x, y, alpha=0.6, label=f'{label} (n={len(subset)})')\n",
    "\n",
    "    # Regression line\n",
    "    plt.plot(x, slope * x + intercept, linewidth=2)\n",
    "\n",
    "    # Print regression statistics\n",
    "    print(f\"{label}: slope={slope:.2f}, intercept={intercept:.2f}, R²={r_value**2:.3f}, p={p_value:.3f}\")\n",
    "\n",
    "plt.title('Cholesterol vs Age by Target with Regression Lines')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Cholesterol (chol)')\n",
    "plt.legend(title='Target')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb240a-f7c3-44e6-b446-0e5963c8879e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1417cc20-a8a9-4808-ab95-9bba110458dd",
   "metadata": {},
   "source": [
    "**PROMPT: Can you show me how to check if there is statistical significance between men and women for each variable?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11434f31-bf82-4215-a0c5-392d5e57987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# Identify continuous columns\n",
    "continuous_cols = [col for col in df.select_dtypes(include='number').columns if df[col].nunique() > 10]\n",
    "\n",
    "# Split data by sex\n",
    "male_df = df[df['sex'] == 'Male']\n",
    "female_df = df[df['sex'] == 'Female']\n",
    "\n",
    "# Run independent t-tests\n",
    "t_results = []\n",
    "for col in continuous_cols:\n",
    "    t_stat, p_val = ttest_ind(male_df[col].dropna(), female_df[col].dropna(), equal_var=False)  # Welch’s t-test\n",
    "    t_results.append({'Variable': col, 't-statistic': t_stat, 'p-value': p_val})\n",
    "\n",
    "t_results_df = pd.DataFrame(t_results)\n",
    "t_results_df['Significant (p<0.05)'] = t_results_df['p-value'] < 0.05\n",
    "display(t_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac053d41-2cc7-4097-baa4-269189f9ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Identify categorical/discrete columns\n",
    "categorical_cols = [col for col in df.columns if df[col].dtype == 'object' or df[col].nunique() <= 10]\n",
    "categorical_cols = [c for c in categorical_cols if c != 'sex']  # exclude sex itself\n",
    "\n",
    "chi_results = []\n",
    "for col in categorical_cols:\n",
    "    contingency = pd.crosstab(df['sex'], df[col])\n",
    "    chi2, p_val, dof, exp = chi2_contingency(contingency)\n",
    "    chi_results.append({'Variable': col, 'Chi²': chi2, 'p-value': p_val})\n",
    "\n",
    "chi_results_df = pd.DataFrame(chi_results)\n",
    "chi_results_df['Significant (p<0.05)'] = chi_results_df['p-value'] < 0.05\n",
    "display(chi_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd6443-390e-4253-a766-ca01dc26ecc2",
   "metadata": {},
   "source": [
    "**PROMPT: How to create two tables containing general statistics: total, men, women. Number and percentage for categorical variables, median and IQR range for continuous variables? Calculate the “p-value” in each table. All floating point numbers should be rounded to three decimal places.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705da03-4b37-44ae-af67-16d8fb58874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency, mannwhitneyu\n",
    "from IPython.display import display\n",
    "\n",
    "# -----------------------\n",
    "# Identify variable types\n",
    "# -----------------------\n",
    "categorical_cols = [col for col in df.columns if df[col].dtype == 'object' or df[col].nunique() <= 10]\n",
    "categorical_cols = [c for c in categorical_cols if c != 'sex']  # exclude sex itself\n",
    "continuous_cols = [col for col in df.select_dtypes(include='number').columns if df[col].nunique() > 10]\n",
    "\n",
    "# Split data by sex\n",
    "male_df = df[df['sex'] == 'Male']\n",
    "female_df = df[df['sex'] == 'Female']\n",
    "\n",
    "# ========================\n",
    "# Table 1 — Categorical\n",
    "# ========================\n",
    "cat_summary = []\n",
    "for col in categorical_cols:\n",
    "    counts = df[col].value_counts(dropna=False)\n",
    "    total_count = counts.sum()\n",
    "\n",
    "    # Chi-square test\n",
    "    contingency = pd.crosstab(df['sex'], df[col])\n",
    "    chi2, p_val, dof, exp = chi2_contingency(contingency)\n",
    "\n",
    "    for category in counts.index:\n",
    "        n_total = counts[category]\n",
    "        n_male = male_df[col].value_counts(dropna=False).get(category, 0)\n",
    "        n_female = female_df[col].value_counts(dropna=False).get(category, 0)\n",
    "\n",
    "        cat_summary.append({\n",
    "            'Variable': col,\n",
    "            'Category': category,\n",
    "            'Total (n,%)': f\"{n_total} ({n_total/total_count*100:.3f}%)\",\n",
    "            'Men (n,%)': f\"{n_male} ({n_male/len(male_df)*100:.3f}%)\",\n",
    "            'Women (n,%)': f\"{n_female} ({n_female/len(female_df)*100:.3f}%)\",\n",
    "            'p-value': round(p_val, 3)\n",
    "        })\n",
    "\n",
    "cat_summary_df = pd.DataFrame(cat_summary)\n",
    "display(cat_summary_df)\n",
    "\n",
    "# ========================\n",
    "# Table 2 — Continuous\n",
    "# ========================\n",
    "cont_summary = []\n",
    "for col in continuous_cols:\n",
    "    # Calculate median and IQR\n",
    "    med_total = df[col].median()\n",
    "    q1_total, q3_total = df[col].quantile([0.25, 0.75])\n",
    "    med_male = male_df[col].median()\n",
    "    q1_male, q3_male = male_df[col].quantile([0.25, 0.75])\n",
    "    med_female = female_df[col].median()\n",
    "    q1_female, q3_female = female_df[col].quantile([0.25, 0.75])\n",
    "\n",
    "    # Mann–Whitney U test (non-parametric)\n",
    "    u_stat, p_val = mannwhitneyu(male_df[col].dropna(), female_df[col].dropna(), alternative='two-sided')\n",
    "\n",
    "    cont_summary.append({\n",
    "        'Variable': col,\n",
    "        'Total (median [IQR])': f\"{med_total:.3f} [{q1_total:.3f}–{q3_total:.3f}]\",\n",
    "        'Men (median [IQR])': f\"{med_male:.3f} [{q1_male:.3f}–{q3_male:.3f}]\",\n",
    "        'Women (median [IQR])': f\"{med_female:.3f} [{q1_female:.3f}–{q3_female:.3f}]\",\n",
    "        'p-value': round(p_val, 3)\n",
    "    })\n",
    "\n",
    "cont_summary_df = pd.DataFrame(cont_summary)\n",
    "display(cont_summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f65ef-4c7b-4790-acb7-ce5c3ead7e30",
   "metadata": {},
   "source": [
    "**PROMPT: How to make a \"p\" correction? Justify your choice of method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43824bdd-9e9c-4d60-a807-f5af9f2dc9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Before correction, keep only one p-value per categorical variable\n",
    "unique_pvals = pd.concat([\n",
    "    cont_summary_df[['Variable', 'p-value']],\n",
    "    cat_summary_df[['Variable', 'p-value']].drop_duplicates('Variable')\n",
    "], ignore_index=True)\n",
    "\n",
    "# Apply Benjamini–Hochberg correction\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "reject, pvals_corrected, _, _ = multipletests(unique_pvals['p-value'], method='fdr_bh')\n",
    "\n",
    "unique_pvals['p-corrected'] = pvals_corrected.round(3)\n",
    "\n",
    "# Merge back to original tables\n",
    "cont_summary_df = cont_summary_df.merge(unique_pvals[['Variable', 'p-corrected']], on='Variable', how='left')\n",
    "cat_summary_df = cat_summary_df.merge(unique_pvals[['Variable', 'p-corrected']], on='Variable', how='left')\n",
    "\n",
    "\n",
    "display(cont_summary_df)\n",
    "display(cat_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b32c08-5052-4f28-892f-99a862a275b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Discovering risk factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea04715f-3a7c-48ae-aa3d-797b4c2686ce",
   "metadata": {},
   "source": [
    "**PROMPT: How to discover risk factors for my \"target\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043ea27-9ca2-4932-b20a-c49bcd9b2abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu, chi2_contingency\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Identify variable types\n",
    "categorical_cols = [col for col in df.columns if df[col].dtype == 'object' or df[col].nunique() <= 10]\n",
    "categorical_cols = [c for c in categorical_cols if c not in ['target']]  # exclude target\n",
    "continuous_cols = [col for col in df.select_dtypes(include='number').columns if df[col].nunique() > 10]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Continuous variables: Mann–Whitney U test\n",
    "for col in continuous_cols:\n",
    "    group1 = df[df['target'] == 0][col].dropna()\n",
    "    group2 = df[df['target'] == 1][col].dropna()\n",
    "    stat, p_val = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "    results.append({'Variable': col, 'Type': 'Continuous', 'p-value': p_val})\n",
    "\n",
    "# Categorical variables: Chi-square test\n",
    "for col in categorical_cols:\n",
    "    contingency = pd.crosstab(df[col], df['target'])\n",
    "    chi2, p_val, dof, exp = chi2_contingency(contingency)\n",
    "    results.append({'Variable': col, 'Type': 'Categorical', 'p-value': p_val})\n",
    "\n",
    "univariate_df = pd.DataFrame(results)\n",
    "univariate_df['Significant (p<0.05)'] = univariate_df['p-value'] < 0.05\n",
    "display(univariate_df.sort_values('p-value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3932fe49-b4b5-423e-ba53-d93acfe11139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Encode categorical variables (one-hot encoding)\n",
    "X = pd.get_dummies(df.drop(columns=['target', 'sex']), drop_first=True)\n",
    "y = df['target']\n",
    "\n",
    "# Add constant term for intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit logistic regression\n",
    "model = sm.Logit(y, X).fit()\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97236850-8319-4a65-a975-db858fcabbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratios = pd.DataFrame({\n",
    "    'Variable': model.params.index,\n",
    "    'Odds Ratio': np.exp(model.params.values),\n",
    "    'p-value': model.pvalues.values\n",
    "})\n",
    "display(odds_ratios.sort_values('p-value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a19a82-08e9-4acf-b858-0ccccd4f8c2e",
   "metadata": {},
   "source": [
    "**PROMPT: How to discover feature importance regarding \"target\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba6fc20-b0c8-4e29-b62f-022b3bf2e127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Encode data\n",
    "X = pd.get_dummies(df.drop(columns=['target', 'sex']), drop_first=True)\n",
    "y = df['target']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance\n",
    "importances = pd.DataFrame({\n",
    "    'Variable': X.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "display(importances)\n",
    "\n",
    "print(\"AUC:\", roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8299afc-6ff4-42f0-b440-d4dd9ff505a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## BONUS. Advanced prompt (don't expect it will produce expected outcome)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b3c483-a3f7-454a-9aba-4bbdd5570fa1",
   "metadata": {},
   "source": [
    "**PROMPT**:\n",
    "\n",
    "I have a cleaned and prepared Heart Disease Cleveland dataset (\"df\") (303 patients, 14 clinical variables). Now I need to conduct exploratory data analysis to formulate evidence-based research hypotheses for epidemiological investigation.\n",
    "\n",
    "Dataset context:\n",
    "- Cleaned dataset ready for analysis\n",
    "- Target: heart disease presence (binary)\n",
    "- 14 clinical predictors including demographics, symptoms, and test results\n",
    "- Balanced analysis needed for hypothesis generation\n",
    "\n",
    "Please create Python code that performs comprehensive exploratory analysis to generate research hypotheses:\n",
    "\n",
    "1. **Univariate Analysis by Heart Disease Status:**\n",
    "   - **Demographics:**\n",
    "     * Age distribution: histograms, box plots, statistical tests (t-test/Mann-Whitney)\n",
    "     * Sex distribution: cross-tabulation, chi-square test\n",
    "     * Calculate mean age difference between diseased/healthy\n",
    "     * Assess age-sex interaction patterns\n",
    "   \n",
    "   - **Clinical Symptoms:**\n",
    "     * Chest pain types (cp) distribution by heart disease status\n",
    "     * Exercise-induced angina (exang) prevalence comparison\n",
    "     * Statistical significance testing for symptom-disease associations\n",
    "   \n",
    "   - **Clinical Measurements:**\n",
    "     * Blood pressure (trestbps): distribution analysis, clinical cut-points\n",
    "     * Cholesterol (chol): distribution analysis, clinical categories\n",
    "     * Maximum heart rate (thalach): age-adjusted analysis\n",
    "     * Fasting blood sugar (fbs) prevalence comparison\n",
    "   \n",
    "   - **Diagnostic Tests:**\n",
    "     * Resting ECG (restecg) abnormalities by disease status\n",
    "     * ST depression (oldpeak) patterns and clinical significance\n",
    "     * ST segment slope (slope) distribution analysis\n",
    "     * Major vessels (ca) involvement patterns\n",
    "     * Thalassemia (thal) defect types by disease status\n",
    "\n",
    "2. **Bivariate Analysis and Correlation:**\n",
    "   - Create comprehensive correlation matrix for numerical variables\n",
    "   - Cross-tabulation tables for categorical variables\n",
    "   - Point-biserial correlations for categorical-numerical pairs\n",
    "   - Identify strongest associations with heart disease\n",
    "   - Clinical interpretation of correlation patterns\n",
    "\n",
    "3. **Advanced Visualization for Hypothesis Generation:**\n",
    "   - **Age-Sex stratified analysis:**\n",
    "     * Heart disease prevalence by age groups and sex\n",
    "     * Age-sex interaction heatmaps\n",
    "     * Stacked bar charts showing disease patterns\n",
    "   \n",
    "   - **Risk Factor Combination Analysis:**\n",
    "     * Multiple chest pain symptoms (cp + exang)\n",
    "     * Blood pressure and cholesterol combined effects\n",
    "     * Age with clinical measurements interactions\n",
    "   \n",
    "   - **Diagnostic Test Patterns:**\n",
    "     * ECG abnormalities with ST depression correlation\n",
    "     * Vessel involvement (ca) with other cardiac markers\n",
    "     * Thalassemia types with disease severity indicators\n",
    "\n",
    "4. **Statistical Evidence Summary for Hypothesis Formulation:**\n",
    "   - Calculate effect sizes (Cohen's d, Cramer's V, odds ratios)\n",
    "   - Identify variables with strongest associations (p<0.05, effect size >0.3)\n",
    "   - Rank variables by predictive potential\n",
    "   - Document clinical plausibility of observed associations\n",
    "\n",
    "5. **Generate Formal Research Hypotheses:**\n",
    "   Based on the exploratory analysis, formulate:\n",
    "   \n",
    "   **Primary Hypothesis:**\n",
    "   - Main research question with strongest evidence\n",
    "   - Specify direction and magnitude of expected effect\n",
    "   \n",
    "   **Secondary Hypotheses (3-4):**\n",
    "   - Additional hypotheses supported by exploratory findings\n",
    "   - Include interaction hypotheses if patterns suggest them\n",
    "   \n",
    "   **Each hypothesis should specify:**\n",
    "   - Null hypothesis (H₀)\n",
    "   - Alternative hypothesis (H₁)\n",
    "   - Expected effect direction and magnitude\n",
    "   - Clinical/biological rationale\n",
    "   - Statistical test to be used for verification\n",
    "   - Required assumptions for the test\n",
    "\n",
    "6. **Create Evidence Summary Tables:**\n",
    "   - Summary table of all associations found (variable, effect size, p-value, clinical interpretation)\n",
    "   - Risk factor ranking by strength of association\n",
    "   - Suggested analysis plan for hypothesis testing\n",
    "   - Power analysis estimates for main hypotheses\n",
    "\n",
    "Please ensure all analyses are clinically grounded and provide epidemiologically meaningful interpretations. Include detailed visualizations that support hypothesis formation and statistical evidence for each proposed hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627efd0-6c04-4122-8c1e-440d41dbeb8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# EDA + HYPOTHESIS GENERATION PIPELINE FOR CLEVELAND HEART DATA\n",
    "# Assumes a cleaned pandas DataFrame `df` is already in memory with standard columns.\n",
    "# Only comments are provided; no prose outside the code.\n",
    "# ============================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from itertools import product, combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from statsmodels.stats.contingency_tables import Table2x2\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.stats.power import TTestIndPower, NormalIndPower\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "np.random.seed(42)\n",
    "\n",
    "TARGET = 'target'  # 0/1\n",
    "# Expected columns in the Cleveland dataset; adjust if your schema differs\n",
    "NUM_COLS = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']  # numerical/continuous\n",
    "CAT_COLS = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']  # categorical/discrete\n",
    "PLOTS_DIR = 'figures'\n",
    "RESULTS_DIR = 'results'\n",
    "ALPHA = 0.05\n",
    "FDR_Q = 0.05\n",
    "\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# UTILITIES\n",
    "# ----------------------------\n",
    "def ensure_numeric(series):\n",
    "    \"\"\"Coerce to numeric for safety.\"\"\"\n",
    "    return pd.to_numeric(series, errors='coerce')\n",
    "\n",
    "def cohen_d_independent(x, y):\n",
    "    \"\"\"Cohen's d for two independent samples (Welch-compatible pooled SD).\"\"\"\n",
    "    x = pd.Series(x).dropna()\n",
    "    y = pd.Series(y).dropna()\n",
    "    nx, ny = len(x), len(y)\n",
    "    if nx < 2 or ny < 2:\n",
    "        return np.nan\n",
    "    sx, sy = x.std(ddof=1), y.std(ddof=1)\n",
    "    # Pooled SD (unbiased); using weighted by df\n",
    "    sp = np.sqrt(((nx - 1) * sx**2 + (ny - 1) * sy**2) / (nx + ny - 2)) if (nx + ny - 2) > 0 else np.nan\n",
    "    if sp == 0 or np.isnan(sp):\n",
    "        return np.nan\n",
    "    return (x.mean() - y.mean()) / sp\n",
    "\n",
    "def rank_biserial_from_u(u_stat, n1, n2):\n",
    "    \"\"\"Rank-biserial correlation from Mann-Whitney U (ranges -1..1).\"\"\"\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return np.nan\n",
    "    return 1 - 2*u_stat/(n1*n2)\n",
    "\n",
    "def cramers_v_from_table(table):\n",
    "    \"\"\"Cramer's V for r x c contingency.\"\"\"\n",
    "    chi2, p, dof, _ = stats.chi2_contingency(table, correction=False)\n",
    "    n = table.values.sum()\n",
    "    r, c = table.shape\n",
    "    v = np.sqrt(chi2 / (n * (min(r, c) - 1))) if min(r, c) > 1 else np.nan\n",
    "    return v, chi2, p, dof\n",
    "\n",
    "def odds_ratio_2x2(table_2x2):\n",
    "    \"\"\"Odds ratio (Fisher exact) for 2x2; Haldane-Anscombe correction if zero cell.\"\"\"\n",
    "    arr = table_2x2.to_numpy().astype(float)\n",
    "    if arr.shape != (2, 2):\n",
    "        return np.nan, np.nan\n",
    "    if (arr == 0).any():\n",
    "        arr = arr + 0.5\n",
    "    or_val = (arr[0,0] * arr[1,1]) / (arr[0,1] * arr[1,0])\n",
    "    _, p = stats.fisher_exact(arr)\n",
    "    return or_val, p\n",
    "\n",
    "def normality_ok(x, alpha=0.05):\n",
    "    \"\"\"Shapiro-Wilk for n between 3 and 5000; returns True if normal.\"\"\"\n",
    "    x = pd.Series(x).dropna()\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return False\n",
    "    _, p = stats.shapiro(x)\n",
    "    return p > alpha\n",
    "\n",
    "def binarize_if_binary(series):\n",
    "    \"\"\"Return boolean mask if the series is binary (two unique values).\"\"\"\n",
    "    vals = pd.Series(series).dropna().unique()\n",
    "    if len(vals) == 2:\n",
    "        mapping = {vals[0]: 0, vals[1]: 1}\n",
    "        return pd.Series(series).map(mapping)\n",
    "    return None\n",
    "\n",
    "def safe_proportions(table_2x2):\n",
    "    \"\"\"Return p1, p2 (proportions) from 2x2 table w.r.t. column=target=1 if columns coded as 0/1.\"\"\"\n",
    "    # Expect rows = exposure (0/1), columns = outcome (0/1)\n",
    "    arr = table_2x2.to_numpy()\n",
    "    if arr.shape != (2, 2):\n",
    "        return np.nan, np.nan\n",
    "    p1 = arr[1,1] / arr[1].sum() if arr[1].sum() > 0 else np.nan\n",
    "    p0 = arr[0,1] / arr[0].sum() if arr[0].sum() > 0 else np.nan\n",
    "    return p1, p0\n",
    "\n",
    "def cut_age_groups(s):\n",
    "    \"\"\"Create clinically sensible age groups.\"\"\"\n",
    "    return pd.cut(s, bins=[-np.inf, 39, 49, 59, 69, np.inf],\n",
    "                  labels=['<40', '40-49', '50-59', '60-69', '70+'])\n",
    "\n",
    "def clinical_bp_cut(s):\n",
    "    \"\"\"Hypertension cutpoint per resting SBP >=140 mmHg.\"\"\"\n",
    "    return pd.Series(np.where(s >= 140, 'HighBP(≥140)', 'NormalBP(<140)'), index=s.index)\n",
    "\n",
    "def clinical_chol_cut(s):\n",
    "    \"\"\"Total cholesterol clinical categories.\"\"\"\n",
    "    cats = pd.cut(s, bins=[-np.inf, 199, 239, np.inf],\n",
    "                  labels=['Desirable(<200)', 'Borderline(200-239)', 'High(≥240)'])\n",
    "    return cats\n",
    "\n",
    "def savefig(path):\n",
    "    \"\"\"Save figure with tight layout.\"\"\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=120)\n",
    "    plt.close()\n",
    "\n",
    "# ----------------------------\n",
    "# 0) BASIC CHECKS\n",
    "# ----------------------------\n",
    "# Ensure required columns exist; will raise if not found\n",
    "missing_needed = [c for c in [TARGET] + NUM_COLS + CAT_COLS if c not in df.columns]\n",
    "if missing_needed:\n",
    "    raise KeyError(f\"Missing expected columns: {missing_needed}\")\n",
    "\n",
    "# Coerce numerics\n",
    "for c in NUM_COLS + [TARGET]:\n",
    "    df[c] = ensure_numeric(df[c])\n",
    "\n",
    "# Ensure categorical dtype\n",
    "for c in CAT_COLS:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype('category')\n",
    "\n",
    "# ----------------------------\n",
    "# 1) UNIVARIATE ANALYSIS BY HEART DISEASE STATUS\n",
    "# ----------------------------\n",
    "\n",
    "# (a) Demographics: Age distribution, boxplots, tests\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "# Histogram by target\n",
    "for t in sorted(df[TARGET].dropna().unique()):\n",
    "    ax1.hist(df.loc[df[TARGET] == t, 'age'].dropna(), alpha=0.5, bins=12, label=f\"target={int(t)}\")\n",
    "ax1.set_title('Age distribution by heart disease status')\n",
    "ax1.set_xlabel('Age [years]')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "# Boxplot by target\n",
    "box_data = [df.loc[df[TARGET] == t, 'age'].dropna().values for t in sorted(df[TARGET].dropna().unique())]\n",
    "ax2.boxplot(box_data, labels=[f\"t={int(t)}\" for t in sorted(df[TARGET].dropna().unique())])\n",
    "ax2.set_title('Age by heart disease status')\n",
    "ax2.set_ylabel('Age [years]')\n",
    "\n",
    "savefig(f\"{PLOTS_DIR}/age_by_target.png\")\n",
    "\n",
    "# Statistical tests for age\n",
    "age0 = df.loc[df[TARGET] == 0, 'age'].dropna()\n",
    "age1 = df.loc[df[TARGET] == 1, 'age'].dropna()\n",
    "if normality_ok(age0) and normality_ok(age1):\n",
    "    t_stat, p_age = stats.ttest_ind(age1, age0, equal_var=False)\n",
    "    test_age = 'Welch t-test'\n",
    "    eff_age = cohen_d_independent(age1, age0)\n",
    "    eff_name_age = 'Cohen_d'\n",
    "else:\n",
    "    u_stat, p_age = stats.mannwhitneyu(age1, age0, alternative='two-sided')\n",
    "    test_age = 'Mann-Whitney U'\n",
    "    eff_age = rank_biserial_from_u(u_stat, len(age1), len(age0))\n",
    "    eff_name_age = 'RankBiserial'\n",
    "mean_diff_age = age1.mean() - age0.mean()\n",
    "\n",
    "# (b) Sex distribution: cross-tab + chi-square\n",
    "ct_sex = pd.crosstab(df['sex'], df[TARGET])\n",
    "v_sex, chi2_sex, p_sex, dof_sex = cramers_v_from_table(ct_sex)\n",
    "\n",
    "# (c) Age-Sex interaction (two-way prevalence table)\n",
    "age_groups = cut_age_groups(df['age'])\n",
    "prev_age_sex = pd.crosstab([age_groups, df['sex']], df[TARGET], normalize='index').rename(columns={0:'Prev_noHD',1:'Prev_HD'})\n",
    "prev_age_sex.to_csv(f\"{RESULTS_DIR}/prev_age_sex.csv\", index=True)\n",
    "\n",
    "# (d) Clinical symptoms: cp distribution, exang prevalence, tests\n",
    "ct_cp = pd.crosstab(df['cp'], df[TARGET])\n",
    "v_cp, chi2_cp, p_cp, dof_cp = cramers_v_from_table(ct_cp)\n",
    "\n",
    "ct_exang = pd.crosstab(df['exang'], df[TARGET])\n",
    "v_exang, chi2_exang, p_exang, dof_exang = cramers_v_from_table(ct_exang)\n",
    "\n",
    "# (e) Clinical measurements: trestbps, chol, thalach, fbs\n",
    "# trestbps tests\n",
    "bp0 = df.loc[df[TARGET] == 0, 'trestbps'].dropna()\n",
    "bp1 = df.loc[df[TARGET] == 1, 'trestbps'].dropna()\n",
    "if normality_ok(bp0) and normality_ok(bp1):\n",
    "    t_bp, p_bp = stats.ttest_ind(bp1, bp0, equal_var=False)\n",
    "    test_bp = 'Welch t-test'\n",
    "    eff_bp = cohen_d_independent(bp1, bp0)\n",
    "    eff_name_bp = 'Cohen_d'\n",
    "else:\n",
    "    u_bp, p_bp = stats.mannwhitneyu(bp1, bp0, alternative='two-sided')\n",
    "    test_bp = 'Mann-Whitney U'\n",
    "    eff_bp = rank_biserial_from_u(u_bp, len(bp1), len(bp0))\n",
    "    eff_name_bp = 'RankBiserial'\n",
    "\n",
    "# chol tests\n",
    "ch0 = df.loc[df[TARGET] == 0, 'chol'].dropna()\n",
    "ch1 = df.loc[df[TARGET] == 1, 'chol'].dropna()\n",
    "if normality_ok(ch0) and normality_ok(ch1):\n",
    "    t_ch, p_ch = stats.ttest_ind(ch1, ch0, equal_var=False)\n",
    "    test_ch = 'Welch t-test'\n",
    "    eff_ch = cohen_d_independent(ch1, ch0)\n",
    "    eff_name_ch = 'Cohen_d'\n",
    "else:\n",
    "    u_ch, p_ch = stats.mannwhitneyu(ch1, ch0, alternative='two-sided')\n",
    "    test_ch = 'Mann-Whitney U'\n",
    "    eff_ch = rank_biserial_from_u(u_ch, len(ch1), len(ch0))\n",
    "    eff_name_ch = 'RankBiserial'\n",
    "\n",
    "# thalach tests (age-adjusted visualization later)\n",
    "th0 = df.loc[df[TARGET] == 0, 'thalach'].dropna()\n",
    "th1 = df.loc[df[TARGET] == 1, 'thalach'].dropna()\n",
    "if normality_ok(th0) and normality_ok(th1):\n",
    "    t_th, p_th = stats.ttest_ind(th1, th0, equal_var=False)\n",
    "    test_th = 'Welch t-test'\n",
    "    eff_th = cohen_d_independent(th1, th0)\n",
    "    eff_name_th = 'Cohen_d'\n",
    "else:\n",
    "    u_th, p_th = stats.mannwhitneyu(th1, th0, alternative='two-sided')\n",
    "    test_th = 'Mann-Whitney U'\n",
    "    eff_th = rank_biserial_from_u(u_th, len(th1), len(th0))\n",
    "    eff_name_th = 'RankBiserial'\n",
    "\n",
    "# fbs prevalence comparison\n",
    "ct_fbs = pd.crosstab(df['fbs'], df[TARGET])\n",
    "v_fbs, chi2_fbs, p_fbs, dof_fbs = cramers_v_from_table(ct_fbs)\n",
    "\n",
    "# (f) Diagnostic tests: restecg, oldpeak, slope, ca, thal\n",
    "ct_rest = pd.crosstab(df['restecg'], df[TARGET]); v_rest, chi2_rest, p_rest, dof_rest = cramers_v_from_table(ct_rest)\n",
    "op0 = df.loc[df[TARGET]==0, 'oldpeak'].dropna(); op1 = df.loc[df[TARGET]==1, 'oldpeak'].dropna()\n",
    "if normality_ok(op0) and normality_ok(op1):\n",
    "    t_op, p_op = stats.ttest_ind(op1, op0, equal_var=False); test_op='Welch t-test'; eff_op=cohen_d_independent(op1,op0); eff_name_op='Cohen_d'\n",
    "else:\n",
    "    u_op, p_op = stats.mannwhitneyu(op1, op0, alternative='two-sided'); test_op='Mann-Whitney U'; eff_op=rank_biserial_from_u(u_op,len(op1),len(op0)); eff_name_op='RankBiserial'\n",
    "ct_slope = pd.crosstab(df['slope'], df[TARGET]); v_slope, chi2_slope, p_slope, dof_slope = cramers_v_from_table(ct_slope)\n",
    "ct_ca = pd.crosstab(df['ca'], df[TARGET]); v_ca, chi2_ca, p_ca, dof_ca = cramers_v_from_table(ct_ca)\n",
    "ct_thal = pd.crosstab(df['thal'], df[TARGET]); v_thal, chi2_thal, p_thal, dof_thal = cramers_v_from_table(ct_thal)\n",
    "\n",
    "# Visuals: clinical cut-points for BP and Chol\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "bp_cat = clinical_bp_cut(df['trestbps'])\n",
    "prev_bp = pd.crosstab(bp_cat, df[TARGET], normalize='index')\n",
    "prev_bp.plot(kind='bar', ax=ax, legend=True, rot=0, title='Heart disease prevalence by BP category')\n",
    "ax.set_ylabel('Proportion')\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "chol_cat = clinical_chol_cut(df['chol'])\n",
    "prev_chol = pd.crosstab(chol_cat, df[TARGET], normalize='index')\n",
    "prev_chol.plot(kind='bar', ax=ax2, legend=True, rot=0, title='Heart disease prevalence by Chol category')\n",
    "ax2.set_ylabel('Proportion')\n",
    "\n",
    "savefig(f\"{PLOTS_DIR}/prevalence_bp_chol.png\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) BIVARIATE + CORRELATION\n",
    "# ----------------------------\n",
    "# Numeric-numeric correlation (Pearson + Spearman)\n",
    "num_df = df[NUM_COLS].apply(pd.to_numeric, errors='coerce')\n",
    "corr_pearson = num_df.corr(method='pearson')\n",
    "corr_spearman = num_df.corr(method='spearman')\n",
    "corr_pearson.to_csv(f\"{RESULTS_DIR}/corr_numeric_pearson.csv\")\n",
    "corr_spearman.to_csv(f\"{RESULTS_DIR}/corr_numeric_spearman.csv\")\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(corr_pearson, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(NUM_COLS)), NUM_COLS, rotation=45, ha='right')\n",
    "plt.yticks(range(len(NUM_COLS)), NUM_COLS)\n",
    "plt.title(\"Pearson correlation (numeric)\")\n",
    "savefig(f\"{PLOTS_DIR}/corr_numeric_pearson.png\")\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(corr_spearman, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(NUM_COLS)), NUM_COLS, rotation=45, ha='right')\n",
    "plt.yticks(range(len(NUM_COLS)), NUM_COLS)\n",
    "plt.title(\"Spearman correlation (numeric)\")\n",
    "savefig(f\"{PLOTS_DIR}/corr_numeric_spearman.png\")\n",
    "\n",
    "# Point-biserial correlations: target (binary) vs each numeric\n",
    "pb_results = []\n",
    "for c in NUM_COLS:\n",
    "    x = df[c]\n",
    "    y = df[TARGET]\n",
    "    mask = x.notna() & y.notna()\n",
    "    if mask.sum() >= 3:\n",
    "        r_pb, p_pb = stats.pointbiserialr(y[mask], x[mask])\n",
    "        pb_results.append({'var': c, 'pointbiserial_r': r_pb, 'p_value': p_pb, 'n': int(mask.sum())})\n",
    "pb_df = pd.DataFrame(pb_results).sort_values('p_value')\n",
    "pb_df.to_csv(f\"{RESULTS_DIR}/pointbiserial_target_numeric.csv\", index=False)\n",
    "\n",
    "# Cross-tab for categorical vs target\n",
    "xtabs = {}\n",
    "for c in CAT_COLS:\n",
    "    xtabs[c] = pd.crosstab(df[c], df[TARGET], margins=False)\n",
    "    xtabs[c].to_csv(f\"{RESULTS_DIR}/crosstab_{c}_vs_target.csv\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) ADVANCED VISUALIZATION FOR HYPOTHESIS GENERATION\n",
    "# ----------------------------\n",
    "\n",
    "# (a) Age-Sex stratified prevalence and heatmap\n",
    "age_cat = cut_age_groups(df['age'])\n",
    "prev_table = pd.crosstab([age_cat, df['sex']], df[TARGET], normalize='index').rename(columns={1:'Prev_HD'}).fillna(0)\n",
    "prev_pivot = prev_table['Prev_HD'].unstack('sex').fillna(0)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.imshow(prev_pivot, aspect='auto')\n",
    "plt.colorbar(label='Prevalence of heart disease')\n",
    "plt.xticks(range(prev_pivot.shape[1]), list(prev_pivot.columns))\n",
    "plt.yticks(range(prev_pivot.shape[0]), list(prev_pivot.index))\n",
    "plt.title('Heart disease prevalence by Age group and Sex')\n",
    "savefig(f\"{PLOTS_DIR}/prevalence_heatmap_age_sex.png\")\n",
    "prev_pivot.to_csv(f\"{RESULTS_DIR}/prevalence_by_age_sex.csv\")\n",
    "\n",
    "# Stacked bar by age groups and sex\n",
    "stack_df = pd.crosstab([age_cat, df['sex']], df[TARGET], normalize='index')\n",
    "stack_df.plot(kind='bar', stacked=True, figsize=(9,4), title='Stacked prevalence by Age group & Sex')\n",
    "plt.ylabel('Proportion')\n",
    "savefig(f\"{PLOTS_DIR}/stacked_prevalence_age_sex.png\")\n",
    "\n",
    "# (b) Risk factor combinations: cp + exang; BP + Chol; Age x Thalach\n",
    "# cp + exang\n",
    "comb_cp_ex = pd.crosstab(index=[df['cp'].astype(str), df['exang'].astype(str)], columns=df[TARGET], normalize='index')\n",
    "comb_cp_ex.to_csv(f\"{RESULTS_DIR}/comb_cp_exang_vs_target.csv\")\n",
    "comb_cp_ex.plot(kind='bar', stacked=True, figsize=(10,4), title='Prevalence by cp + exang combinations')\n",
    "plt.ylabel('Proportion')\n",
    "savefig(f\"{PLOTS_DIR}/comb_cp_exang.png\")\n",
    "\n",
    "# BP + Chol (clinical categories)\n",
    "bp_cat = clinical_bp_cut(df['trestbps'])\n",
    "chol_cat = clinical_chol_cut(df['chol'])\n",
    "comb_bp_chol = pd.crosstab(index=[bp_cat, chol_cat], columns=df[TARGET], normalize='index')\n",
    "comb_bp_chol.to_csv(f\"{RESULTS_DIR}/comb_bp_chol_vs_target.csv\")\n",
    "comb_bp_chol.plot(kind='bar', stacked=True, figsize=(10,4), title='Prevalence by BP + Chol categories')\n",
    "plt.ylabel('Proportion')\n",
    "savefig(f\"{PLOTS_DIR}/comb_bp_chol.png\")\n",
    "\n",
    "# Age group x thalach (low/high by median)\n",
    "thalach_cut = pd.Series(np.where(df['thalach'] >= df['thalach'].median(), 'HighThalach', 'LowThalach'))\n",
    "comb_age_thalach = pd.crosstab(index=[age_cat, thalach_cut], columns=df[TARGET], normalize='index')\n",
    "comb_age_thalach.to_csv(f\"{RESULTS_DIR}/comb_age_thalach_vs_target.csv\")\n",
    "comb_age_thalach.plot(kind='bar', stacked=True, figsize=(10,4), title='Prevalence by Age group x Thalach')\n",
    "plt.ylabel('Proportion')\n",
    "savefig(f\"{PLOTS_DIR}/comb_age_thalach.png\")\n",
    "\n",
    "# (c) Diagnostic test patterns\n",
    "# ECG abnormalities vs oldpeak (scatter/jitter)\n",
    "plt.figure(figsize=(7,4))\n",
    "x_ecg = df['restecg'].cat.codes\n",
    "y_oldpeak = df['oldpeak']\n",
    "plt.scatter(x_ecg + (np.random.rand(len(x_ecg))-0.5)*0.15, y_oldpeak, alpha=0.6)\n",
    "plt.xticks(sorted(x_ecg.unique()), list(df['restecg'].cat.categories))\n",
    "plt.xlabel('restecg category')\n",
    "plt.ylabel('oldpeak (ST depression)')\n",
    "plt.title('oldpeak by restecg category (jittered)')\n",
    "savefig(f\"{PLOTS_DIR}/oldpeak_by_restecg_scatter.png\")\n",
    "\n",
    "# Vessel involvement (ca) vs prevalence\n",
    "prev_ca = pd.crosstab(df['ca'], df[TARGET], normalize='index')\n",
    "prev_ca.plot(kind='bar', stacked=True, rot=0, figsize=(6,4), title='Prevalence by number of major vessels (ca)')\n",
    "plt.ylabel('Proportion')\n",
    "savefig(f\"{PLOTS_DIR}/prevalence_by_ca.png\")\n",
    "\n",
    "# Thal types vs oldpeak distribution\n",
    "plt.figure(figsize=(7,4))\n",
    "data = [df.loc[df['thal'] == k, 'oldpeak'].dropna().values for k in df['thal'].cat.categories]\n",
    "plt.boxplot(data, labels=list(df['thal'].cat.categories))\n",
    "plt.ylabel('oldpeak')\n",
    "plt.title('oldpeak by thal category')\n",
    "savefig(f\"{PLOTS_DIR}/oldpeak_by_thal_box.png\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) STATISTICAL EVIDENCE SUMMARY (EFFECT SIZES, FDR, RANKING)\n",
    "# ----------------------------\n",
    "results = []\n",
    "\n",
    "# Continuous vs target loop\n",
    "for c in NUM_COLS:\n",
    "    g0 = df.loc[df[TARGET]==0, c].dropna()\n",
    "    g1 = df.loc[df[TARGET]==1, c].dropna()\n",
    "    if len(g0) >= 3 and len(g1) >= 3:\n",
    "        if normality_ok(g0) and normality_ok(g1):\n",
    "            stat, p = stats.ttest_ind(g1, g0, equal_var=False)\n",
    "            test = 'Welch t-test'\n",
    "            eff = cohen_d_independent(g1, g0)\n",
    "            eff_name = 'Cohen_d'\n",
    "            direction = 'higher_in_HD' if g1.mean() > g0.mean() else 'lower_in_HD'\n",
    "        else:\n",
    "            u, p = stats.mannwhitneyu(g1, g0, alternative='two-sided')\n",
    "            test = 'Mann-Whitney U'\n",
    "            eff = rank_biserial_from_u(u, len(g1), len(g0))\n",
    "            eff_name = 'RankBiserial'\n",
    "            direction = 'higher_in_HD' if np.median(g1) > np.median(g0) else 'lower_in_HD'\n",
    "        results.append({'variable': c, 'type': 'numeric', 'test': test, 'stat': stat if test.startswith('Welch') else u,\n",
    "                        'p_value': p, 'effect_name': eff_name, 'effect_size': eff, 'direction': direction,\n",
    "                        'n0': len(g0), 'n1': len(g1)})\n",
    "\n",
    "# Categorical vs target loop\n",
    "for c in CAT_COLS:\n",
    "    tab = pd.crosstab(df[c], df[TARGET])\n",
    "    if tab.values.sum() == 0 or tab.shape[0] < 1 or tab.shape[1] < 1:\n",
    "        continue\n",
    "    v, chi2, p, dof = cramers_v_from_table(tab)\n",
    "    results.append({'variable': c, 'type': 'categorical', 'test': 'Chi-square', 'stat': chi2,\n",
    "                    'p_value': p, 'effect_name': 'Cramers_V', 'effect_size': v, 'direction': 'n/a',\n",
    "                    'n0': int(tab[0].sum()) if 0 in tab.columns else np.nan,\n",
    "                    'n1': int(tab[1].sum()) if 1 in tab.columns else np.nan})\n",
    "    # Add odds ratio for binary 2x2 only\n",
    "    if tab.shape == (2,2):\n",
    "        or_val, p_or = odds_ratio_2x2(tab)\n",
    "        results.append({'variable': f'{c} (OR 2x2)', 'type': 'categorical', 'test': 'Fisher exact',\n",
    "                        'stat': np.nan, 'p_value': p_or, 'effect_name': 'OddsRatio', 'effect_size': or_val,\n",
    "                        'direction': 'OR>1 favors HD', 'n0': int(tab[0].sum()), 'n1': int(tab[1].sum())})\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "# FDR within numeric and within categorical families (optional); here global FDR\n",
    "res_df['p_adj_BH'] = multipletests(res_df['p_value'], method='fdr_bh')[1]\n",
    "res_df['sig_FDR'] = res_df['p_adj_BH'] < FDR_Q\n",
    "res_df.to_csv(f\"{RESULTS_DIR}/evidence_summary_all.csv\", index=False)\n",
    "\n",
    "# Rank by predictive potential: significant & |effect|>0.3\n",
    "rank_df = (res_df\n",
    "           .assign(abs_eff=lambda d: d['effect_size'].abs())\n",
    "           .sort_values(['sig_FDR','abs_eff','p_adj_BH'], ascending=[False, False, True]))\n",
    "rank_df.to_csv(f\"{RESULTS_DIR}/ranked_variables.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) GENERATE FORMAL RESEARCH HYPOTHESES (PROGRAMMATIC TEMPLATES)\n",
    "# ----------------------------\n",
    "# Select top candidates from numeric (by |effect|) and categorical (by Cramer's V / OR)\n",
    "top_numeric = (res_df[(res_df['type']=='numeric') & res_df['sig_FDR']]\n",
    "               .sort_values('effect_size', key=np.abs, ascending=False).head(3))\n",
    "top_categorical = (res_df[(res_df['type']=='categorical') & res_df['sig_FDR'] & (res_df['effect_name']=='Cramers_V')]\n",
    "                   .sort_values('effect_size', ascending=False).head(3))\n",
    "\n",
    "hypotheses = []\n",
    "\n",
    "# Primary hypothesis: choose strongest signal overall\n",
    "if not rank_df.empty:\n",
    "    primary = rank_df.iloc[0]\n",
    "    var = primary['variable']\n",
    "    eff = primary['effect_size']\n",
    "    p = primary['p_adj_BH']\n",
    "    if primary['type'] == 'numeric':\n",
    "        direction = 'increases' if eff > 0 else 'decreases'\n",
    "        test_name = 'Welch t-test or Mann-Whitney (as appropriate); logistic regression for confirmation'\n",
    "        effect_desc = f\"expected standardized effect ≈ {eff:.2f}\"\n",
    "    else:\n",
    "        direction = 'associated with'\n",
    "        test_name = 'Chi-square (or Fisher for sparse), logistic regression'\n",
    "        effect_desc = f\"expected association strength (Cramer\\'s V) ≈ {eff:.2f}\"\n",
    "    hypotheses.append({\n",
    "        'title': f\"Primary: {var} {direction} heart disease risk\",\n",
    "        'H0': f\"No association between {var} and heart disease.\",\n",
    "        'H1': f\"{var} is {direction} heart disease risk; {effect_desc}.\",\n",
    "        'test': test_name,\n",
    "        'alpha': ALPHA,\n",
    "        'FDR_q': FDR_Q,\n",
    "        'note': f\"Selected by strongest effect (FDR-adjusted p={p:.3g}).\"\n",
    "    })\n",
    "\n",
    "# Secondary hypotheses from remaining top signals\n",
    "for row in pd.concat([top_numeric, top_categorical]).head(4).itertuples():\n",
    "    var = row.variable\n",
    "    eff = row.effect_size\n",
    "    if row.type == 'numeric':\n",
    "        direction = 'higher in diseased' if row.direction=='higher_in_HD' else 'lower in diseased'\n",
    "        test_name = 'Welch t-test or Mann-Whitney; confirm via logistic regression with age/sex adjustment'\n",
    "        effect_desc = f\"expected effect size ≈ {eff:.2f}\"\n",
    "    else:\n",
    "        direction = 'associated with disease status'\n",
    "        test_name = 'Chi-square/Fisher; confirm via logistic regression'\n",
    "        effect_desc = f\"expected Cramer\\'s V ≈ {eff:.2f}\"\n",
    "    hypotheses.append({\n",
    "        'title': f\"Secondary: {var} {direction}\",\n",
    "        'H0': f\"No association between {var} and disease status.\",\n",
    "        'H1': f\"{var} is {direction}; {effect_desc}.\",\n",
    "        'test': test_name,\n",
    "        'alpha': ALPHA,\n",
    "        'FDR_q': FDR_Q,\n",
    "        'note': \"Derived from EDA signals.\"\n",
    "    })\n",
    "\n",
    "# vals: matrix of prevalence values (float), handle possible NaNs safely\n",
    "vals = np.asarray(prev_pivot.values, dtype=float)\n",
    "\n",
    "# range across cells (nan-safe); triggers interaction hypothesis if > 0.15\n",
    "rng = np.nanmax(vals) - np.nanmin(vals) if vals.size else 0.0\n",
    "\n",
    "if vals.size and np.isfinite(rng) and rng > 0.15:\n",
    "    hypotheses.append({\n",
    "        'title': \"Secondary (Interaction): Age × Sex modifies heart disease prevalence\",\n",
    "        'H0': \"No interaction between age group and sex on heart disease prevalence.\",\n",
    "        'H1': \"Prevalence increases with age more steeply in one sex (interaction present).\",\n",
    "        'test': \"Logistic regression with AgeGroup, Sex, and AgeGroup×Sex term; LRT for interaction\",\n",
    "        'alpha': ALPHA,\n",
    "        'FDR_q': FDR_Q,\n",
    "        'note': \"Triggered by heatmap heterogeneity.\"\n",
    "    })\n",
    "\n",
    "# Export hypotheses as a human-readable CSV/text\n",
    "hypo_df = pd.DataFrame(hypotheses)\n",
    "hypo_df.to_csv(f\"{RESULTS_DIR}/hypotheses_generated.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) EVIDENCE SUMMARY TABLES + POWER ANALYSIS\n",
    "# ----------------------------\n",
    "# Assemble a compact summary table with clinical-style interpretation placeholders\n",
    "summary_rows = []\n",
    "for row in res_df.itertuples():\n",
    "    var = row.variable\n",
    "    eff = row.effect_size\n",
    "    p_adj = row.p_adj_BH\n",
    "    interpretable = ''\n",
    "    if row.type == 'numeric' and not np.isnan(eff):\n",
    "        interpretable = f\"{row.effect_name}={eff:.2f} ({row.direction})\"\n",
    "    elif row.effect_name == 'Cramers_V' and not np.isnan(eff):\n",
    "        interpretable = f\"Cramér's V={eff:.2f} (association)\"\n",
    "    elif row.effect_name == 'OddsRatio' and not np.isnan(eff):\n",
    "        interpretable = f\"OR≈{eff:.2f} (>1 suggests higher odds in exposed)\"\n",
    "    summary_rows.append({\n",
    "        'variable': var,\n",
    "        'type': row.type,\n",
    "        'test_used': row.test,\n",
    "        'stat_or_effect': interpretable,\n",
    "        'p_value': row.p_value,\n",
    "        'p_adj_BH': p_adj,\n",
    "        'significant_FDR': row.sig_FDR\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(['significant_FDR','p_adj_BH'], ascending=[False, True])\n",
    "summary_df.to_csv(f\"{RESULTS_DIR}/evidence_summary_clinical.csv\", index=False)\n",
    "\n",
    "# Rank by strength (significant and |effect|>0.3)\n",
    "rank_filtered = (res_df.assign(abs_eff=lambda d: d['effect_size'].abs())\n",
    "                 .query('sig_FDR and abs_eff > 0.3')\n",
    "                 .sort_values(['abs_eff','p_adj_BH'], ascending=[False, True]))\n",
    "rank_filtered[['variable','effect_name','effect_size','p_adj_BH']].to_csv(f\"{RESULTS_DIR}/risk_factor_ranking.csv\", index=False)\n",
    "\n",
    "# Power analysis for primary numeric and 2x2 hypotheses (rough, based on observed effect sizes)\n",
    "power_results = []\n",
    "\n",
    "# Numeric (two-sample t-test) example: use strongest numeric signal\n",
    "num_sig = res_df[(res_df['type']=='numeric') & res_df['sig_FDR']].copy()\n",
    "if not num_sig.empty:\n",
    "    best = num_sig.iloc[num_sig['effect_size'].abs().argmax()]\n",
    "    d = np.nan_to_num(best['effect_size'])\n",
    "    # Estimate group sizes from dataset\n",
    "    n0 = int((df[TARGET]==0).sum())\n",
    "    n1 = int((df[TARGET]==1).sum())\n",
    "    ratio = n1 / max(n0,1)\n",
    "    tpower = TTestIndPower()\n",
    "    # Required per-group n for 80% power\n",
    "    try:\n",
    "        n_per_group = math.ceil(tpower.solve_power(effect_size=abs(d), alpha=ALPHA, power=0.80, ratio=ratio))\n",
    "    except Exception:\n",
    "        n_per_group = np.nan\n",
    "    # Achieved power with current sample\n",
    "    try:\n",
    "        achieved = tpower.power(effect_size=abs(d), alpha=ALPHA, nobs1=n0, ratio=ratio)\n",
    "    except Exception:\n",
    "        achieved = np.nan\n",
    "    power_results.append({'hypothesis': f'Numeric: {best.variable}', 'effect': d, 'required_n_per_group_80pct': n_per_group, 'achieved_power': achieved})\n",
    "\n",
    "# Binary 2x2 example: pick a binary categorical with strongest OR signal if available\n",
    "or_rows = res_df[res_df['effect_name']=='OddsRatio'].copy()\n",
    "if not or_rows.empty:\n",
    "    or_best = or_rows.iloc[or_rows['p_adj_BH'].argmin()]\n",
    "    # Reconstruct 2x2 and compute p1, p0\n",
    "    var = or_best['variable'].replace(' (OR 2x2)','')\n",
    "    tab = pd.crosstab(df[var], df[TARGET])\n",
    "    if tab.shape == (2,2):\n",
    "        p1, p0 = safe_proportions(tab)\n",
    "        if np.isfinite(p1) and np.isfinite(p0):\n",
    "            es = proportion_effectsize(p1, p0)  # Cohen's h\n",
    "            npower = NormalIndPower()\n",
    "            try:\n",
    "                n_per_group = math.ceil(npower.solve_power(effect_size=abs(es), alpha=ALPHA, power=0.80, ratio=1.0))\n",
    "            except Exception:\n",
    "                n_per_group = np.nan\n",
    "            try:\n",
    "                achieved = npower.power(effect_size=abs(es), alpha=ALPHA, nobs1=int(tab.iloc[0].sum()), ratio=1.0)\n",
    "            except Exception:\n",
    "                achieved = np.nan\n",
    "            power_results.append({'hypothesis': f'Binary: {var}', 'effect_Cohen_h': es, 'required_n_per_group_80pct': n_per_group, 'achieved_power': achieved})\n",
    "\n",
    "pd.DataFrame(power_results).to_csv(f\"{RESULTS_DIR}/power_analysis_estimates.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# PRINT KEY SUMMARIES (OPTIONAL)\n",
    "# ----------------------------\n",
    "print(\"Saved:\")\n",
    "print(f\"- Figures to: {PLOTS_DIR}/\")\n",
    "print(f\"- Numeric correlations: {RESULTS_DIR}/corr_numeric_*.csv\")\n",
    "print(f\"- Crosstabs: {RESULTS_DIR}/crosstab_*_vs_target.csv\")\n",
    "print(f\"- Evidence summary: {RESULTS_DIR}/evidence_summary_all.csv\")\n",
    "print(f\"- Ranked variables: {RESULTS_DIR}/ranked_variables.csv\")\n",
    "print(f\"- Hypotheses: {RESULTS_DIR}/hypotheses_generated.csv\")\n",
    "print(f\"- Power analysis: {RESULTS_DIR}/power_analysis_estimates.csv\")\n",
    "\n",
    "# ----------------------------\n",
    "# NOTES (in-code, non-executable):\n",
    "# - Effect-size thresholds: consider |d|≥0.3 (small-moderate), Cramér’s V≥0.3 as meaningful screens for hypotheses.\n",
    "# - Odds ratios computed only for binary 2x2 tables. For multi-level factors, use logistic regression to estimate per-level OR (ref vs baseline).\n",
    "# - For thalach age-adjustment beyond visuals, fit a simple logistic regression (target ~ thalach + age) as confirmatory step later.\n",
    "# - Generated hypotheses are templates filled with observed directions/effects and are meant to guide confirmatory modeling, not serve as final proof.\n",
    "# ============================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
