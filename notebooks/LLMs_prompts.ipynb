{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38eb386d-aaa1-4b89-bbcd-9d3f05b3628f",
   "metadata": {},
   "source": [
    "# LLMs prompts\n",
    "\n",
    "School of Epidemiology\n",
    "\n",
    "Futuri Center, Bia≈Çystok, October 22, 2025 \n",
    "\n",
    "Wojciech ≈Åaguna\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54165eaf-17a8-4c6c-bae5-ec5eac11aa8f",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">Warning!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3277b3-dd10-4196-ad5e-096a7c2ecb7d",
   "metadata": {},
   "source": [
    "The following materials are for data review purposes only. The results of the prompts **should not be used in scientific work under any circumstances**. The prompts should only be used for preliminary data review. Final statistics and data analysis should be performed by a professional statistician. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7bff00-cf7d-4d52-ae0e-aa2202c8913c",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d09980-c38e-4b58-ae92-aedfb2d63adf",
   "metadata": {},
   "source": [
    "1. [Anaconda](https://www.anaconda.com/download/success) (preferred) or stanalone [Jupyter Notebook](https://jupyter.org/install) installed\n",
    "\n",
    "2. Jupyter Lab selected (right-up corner of the image)\n",
    "  \n",
    "3. Access to **one of the following LLMs** (free or paid version). In some you need to be registered:\n",
    "   - [Claude](https://claude.ai/login?returnTo=%2F%3F)\n",
    "   - [ChatGPT](https://chatgpt.com/)\n",
    "   - [Gemini](https://gemini.google.com/)\n",
    "   - [GROK](https://grok.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbbc2e3-584b-4354-b086-9ec7d51a8fa8",
   "metadata": {},
   "source": [
    "## Data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292d19d6-d660-4351-b458-53e6d0574e9e",
   "metadata": {},
   "source": [
    "**[Heart Disease Cleveland](https://www.kaggle.com/datasets/ritwikb3/heart-disease-cleveland/data)**\n",
    "\n",
    "The dataset is the Cleveland Heart Disease dataset taken from the UCI repository. The dataset consists of 303 individuals‚Äô data.\n",
    "\n",
    "\n",
    "[Licence:CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1e0418-e2b2-4f07-aeae-afb6940fa656",
   "metadata": {},
   "source": [
    "1. **Age**: Patients Age in years \n",
    "2. **Sex**: Gender (Male : 1; Female : 0) \n",
    "3. **cp**: Type of chest pain experienced by patient. This term categorized into 4 category.\n",
    "    - 0 typical angina,\n",
    "    - 1 atypical angina,\n",
    "    - 2 non-anginal pain,\n",
    "    - 3 asymptomatic \n",
    "4. **trestbps**: patient's level of blood pressure at resting mode in mm/HG \n",
    "5. **chol**: Serum cholesterol in mg/dl \n",
    "6. **fbs**: Blood sugar levels on fasting > 120 mg/dl represents as 1 in case of true and 0 as false \n",
    "7. **restecg**: Result of electrocardiogram while at rest are represented in 3 distinct values\n",
    "   - 0 : Normal\n",
    "   - 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "   - 2: showing probable or definite left ventricular hypertrophyby Estes' criteria \n",
    "8. **thalach**: Maximum heart rate achieved \n",
    "9. **exang**: Angina induced by exercise 0 depicting NO 1 depicting Yes\n",
    "10. **oldpeak**: Exercise induced ST-depression in relative with the state of rest \n",
    "11. **slope**: ST segment measured in terms of slope during peak exercise\n",
    "    - 0: up sloping;\n",
    "    - 1: flat;\n",
    "    - 2: down sloping\n",
    "14. **ca**: The number of major vessels (0‚Äì3)\n",
    "15. **thal**: A blood disorder called thalassemia\n",
    "    - 0: NULL\n",
    "    - 1: normal blood flow\n",
    "    - 2: fixed defect (no blood flow in some part of the heart)\n",
    "    - 3: reversible defect (a blood flow is observed but it is not normal\n",
    "16. ***target***: It is the target variable which we have to predict. 1 means patient is suffering from heart disease and 0 means patient is normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a96abc8-033e-41e9-ae6d-6c4cd9dc7d57",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16acf348-cd4c-4ace-ab3e-f66ef4cddc0d",
   "metadata": {},
   "source": [
    "**PROMPT: I need to load a dataset from given address to pandas './Heart_disease_cleveland_new.csv'. Plase assign it to \"df\" variable**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85665216-4564-4f31-83a8-582e9a9c2a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the local file path into the DataFrame 'df'\n",
    "df = pd.read_csv('./Heart_disease_cleveland_new.csv')\n",
    "\n",
    "# You can display the first few rows to verify the load (optional)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd7f2f3-3fd1-4d58-8a2f-f35aebf345be",
   "metadata": {},
   "source": [
    "**PROMPT: Can I see table in a nicer outline?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d50cb-3ee9-4467-b510-5e7e478d8377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1553eea-fe0d-469f-b85a-867d2b7b07c1",
   "metadata": {},
   "source": [
    "**PROMPT: What types of data do I have in my dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b5622-bfbd-4784-b84c-2a2accaa5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0cc3f-470f-4850-af99-ba6bb3b45762",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ac33b-4788-48d0-ac05-9db156b45cd0",
   "metadata": {},
   "source": [
    "**PROMPT: I know that \"chol\" should be a number, not \"object.\" How can I check if this and other columns displaying the \"object\" type contain a random string?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a07b59-a68f-4200-8616-ce2adf5c611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Select all columns that are currently of 'object' dtype\n",
    "object_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(\"--- Inspection of 'Object' Columns for Non-Numeric Data ---\")\n",
    "\n",
    "if len(object_cols) == 0:\n",
    "    print(\"No 'object' columns found in the DataFrame.\")\n",
    "else:\n",
    "    # 2. Loop through each 'object' column and check for non-numeric values\n",
    "    for col in object_cols:\n",
    "        # Attempt to convert the column to a numeric type.\n",
    "        # errors='coerce' turns any non-convertible string (like '?' or 'abc') into NaN.\n",
    "        numeric_col = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # Count the number of values that turned into NaN during conversion\n",
    "        non_numeric_count = numeric_col.isnull().sum()\n",
    "\n",
    "        if non_numeric_count > 0:\n",
    "            print(f\"\\nüö® Column '{col}' contains {non_numeric_count} non-numeric values.\")\n",
    "\n",
    "            # Find the original unique string values that caused the conversion to fail (NaN)\n",
    "            # We filter the original column where the newly created numeric column is NaN\n",
    "            non_numeric_values = df[col][numeric_col.isna()].unique()\n",
    "\n",
    "            # The set should exclude 'np.nan' if it was already present as a string 'nan'\n",
    "            cleaned_values = [\n",
    "                val for val in non_numeric_values if str(val).lower() != 'nan' and pd.notna(val)\n",
    "            ]\n",
    "\n",
    "            if cleaned_values:\n",
    "                 print(f\"   Unique non-numeric strings found: {cleaned_values}\")\n",
    "            else:\n",
    "                 print(\"   The non-numeric values appear to be empty strings or purely NaN values.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ Column '{col}' contains only numeric data, but is currently stored as 'object'.\")\n",
    "            print(\"   You can safely convert this column to a numeric type (float or integer).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1cce54-d87d-465f-b8ee-f22fb4ad8538",
   "metadata": {},
   "source": [
    "**PROMPT: How can I replace \"unknown\" by an empty value?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b4c19-e915-4aa9-b5b5-d0c53a95bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace the string \"unknown\" with the standard pandas missing value NaN\n",
    "df.replace(\"unknown\", np.nan, inplace=True)\n",
    "\n",
    "# You can verify the change by checking the number of missing values (optional)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da15b04-b216-4437-bf57-cf762d9d61e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483409b6-e236-4357-a611-7ea5a8611b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f908413-4e8b-477b-be01-0c58d0b30746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a82c8c0d-8e2d-4411-a4cb-46c919bf0010",
   "metadata": {},
   "source": [
    "**PROMPT: I replaced, but \"chol\" is still an \"object\" in my dataframe.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dad12c-1f3d-4059-b14b-bc56aff8a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace the common placeholder '?' with NaN\n",
    "df.replace(\"?\", np.nan, inplace=True)\n",
    "\n",
    "# You should also re-run the inspection logic to find any *other* remaining non-numeric strings\n",
    "print(\"\\n--- Remaining Unique Values in 'chol' ---\")\n",
    "# Use the same logic as before to check for non-convertible strings\n",
    "numeric_chol = pd.to_numeric(df['chol'], errors='coerce')\n",
    "non_numeric_count = numeric_chol.isnull().sum()\n",
    "\n",
    "if non_numeric_count > 0:\n",
    "    print(f\"üö® 'chol' still contains {non_numeric_count} non-numeric values (including NaNs).\")\n",
    "    non_numeric_values = df['chol'][numeric_chol.isna()].unique()\n",
    "    \n",
    "    # Filter out actual np.nan values to see the problematic strings\n",
    "    problematic_strings = [\n",
    "        val for val in non_numeric_values if pd.notna(val) and str(val).lower() != 'nan'\n",
    "    ]\n",
    "\n",
    "    if problematic_strings:\n",
    "         print(f\"   Remaining unique problematic strings: {problematic_strings}\")\n",
    "    else:\n",
    "         print(\"   No other specific strings found; conversion should proceed with existing NaNs.\")\n",
    "else:\n",
    "    print(\"‚úÖ 'chol' now appears to contain only numeric values or standard NaNs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd558ce-44c2-4557-925d-dc8d25d7596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert 'chol' column to a numeric type (float64)\n",
    "# errors='coerce' ensures that any remaining non-numeric strings are turned into np.nan\n",
    "df['chol'] = pd.to_numeric(df['chol'], errors='coerce')\n",
    "\n",
    "# Verify the data type\n",
    "print(f\"The new data type for 'chol' is: {df['chol'].dtype}\")\n",
    "\n",
    "# Optional: Check how many non-numeric values were converted to NaN\n",
    "# This tells you how many problematic entries were cleaned up by the conversion\n",
    "print(f\"Number of missing values (NaNs) in 'chol' after conversion: {df['chol'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee47ec51-f6b4-4ee1-b26c-5ad9e7b9fc6d",
   "metadata": {},
   "source": [
    "**PROPMPT: How to discover outliers in my dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc251ba3-102d-497a-80dc-eefa654bd5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate Q1, Q3, and IQR\n",
    "Q1 = df['chol'].quantile(0.25)\n",
    "Q3 = df['chol'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 2. Define the outlier fences\n",
    "lower_bound = Q1 - (1.5 * IQR)\n",
    "upper_bound = Q3 + (1.5 * IQR)\n",
    "\n",
    "# 3. Identify the outliers\n",
    "outliers = df[(df['chol'] < lower_bound) | (df['chol'] > upper_bound)]\n",
    "\n",
    "print(f\"Number of outliers in 'chol': {len(outliers)}\")\n",
    "print(\"Outlier details (first 5):\")\n",
    "print(outliers[['chol']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1d97b-4843-41f9-b164-973addb60b56",
   "metadata": {},
   "source": [
    "**PROMPT: I see that the maximum values ‚Äã‚Äãfor \"chol\" and \"thalach\" are outliers, as are the minimum values ‚Äã‚Äãfor \"target.\" How do I replace them with an empty value?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e5484a-63b2-41ea-963e-fb15916ec57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c871dd5-46b3-4645-814b-fa00ced049ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Dictionary to store the count of outliers for each column\n",
    "outlier_counts = {}\n",
    "\n",
    "print(\"--- Outlier Discovery using 1.5 * IQR Rule ---\")\n",
    "\n",
    "# Loop through every numeric column\n",
    "for col in numeric_cols:\n",
    "    # Drop NaNs before calculating statistics to avoid errors\n",
    "    data = df[col].dropna()\n",
    "\n",
    "    if data.empty:\n",
    "        print(f\"Column '{col}' is empty or all NaN, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define the outlier fences (1.5 * IQR rule)\n",
    "    lower_bound = Q1 - (1.5 * IQR)\n",
    "    upper_bound = Q3 + (1.5 * IQR)\n",
    "\n",
    "    # Count the number of outliers\n",
    "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    count = len(outliers)\n",
    "\n",
    "    outlier_counts[col] = count\n",
    "\n",
    "    if count > 0:\n",
    "        print(f\"üö® Column '{col}': {count} outliers found.\")\n",
    "        # Optional: Print the min/max of the outliers to see their values\n",
    "        print(f\"   Min Outlier: {outliers.min():.2f}, Max Outlier: {outliers.max():.2f}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Column '{col}': No outliers found.\")\n",
    "\n",
    "# Display a summary of all findings\n",
    "print(\"\\n--- Summary of Outliers per Column ---\")\n",
    "summary_df = pd.Series(outlier_counts, name='Outlier Count')\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623b206-fc1e-4908-bcdc-a9f62095a9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9363b4-b802-405d-b0ad-d229e5d11011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Assuming the DataFrame 'df' has been successfully loaded and cleaned ---\n",
    "\n",
    "# 1. Select all numeric columns\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "# Filter out columns that are all NaN after cleaning\n",
    "final_cols_to_plot = [col for col in numeric_cols if not df[col].dropna().empty]\n",
    "\n",
    "if final_cols_to_plot:\n",
    "    num_cols = len(final_cols_to_plot)\n",
    "\n",
    "    # Determine grid size for subplots\n",
    "    n_rows = (num_cols + 2) // 3  # Roughly 3 columns per row\n",
    "    n_cols = 3\n",
    "\n",
    "    # Create figure and subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
    "    axes = axes.flatten() # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "    for i, col in enumerate(final_cols_to_plot):\n",
    "        # Use dropna() to ignore NaN values for the boxplot calculation\n",
    "        axes[i].boxplot(df[col].dropna())\n",
    "        axes[i].set_title(f'{col}', fontsize=12)\n",
    "        # Remove x-axis tick labels for cleaner look on multiple plots\n",
    "        axes[i].tick_params(axis='x', labelbottom=False)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(len(final_cols_to_plot), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.suptitle('Box Plots for Outlier Visualization Across All Numeric Columns', fontsize=16, y=1.02)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "    plt.show() # In a real environment, you'd use plt.show() or plt.savefig()\n",
    "\n",
    "else:\n",
    "    print(\"No suitable numeric columns found for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bc0ba3-dd4d-4193-9ea4-da4e2c3eddac",
   "metadata": {},
   "source": [
    "**PROMPT: How many missing values are in my dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72724827-59b8-47ef-be5a-55c66d9aa7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check for null/missing values across the entire DataFrame\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "print(\"--- Missing Values Count Per Column ---\")\n",
    "print(missing_values)\n",
    "\n",
    "# 2. To get the total count across the entire dataset:\n",
    "total_missing = missing_values.sum()\n",
    "print(f\"\\nTotal missing values in the entire dataset: {total_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cecc8b-32e7-4aab-85f5-931dbfd926ac",
   "metadata": {},
   "source": [
    "**PROMPT: How to remove all records that contain at least one empty value?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15587ad0-f484-4725-84c4-ec254239d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame containing only rows with NO missing values\n",
    "df_cleaned = df.dropna(how='any')\n",
    "\n",
    "# Optional: Print the original and new number of rows to see how many were removed\n",
    "print(f\"Original number of records: {len(df)}\")\n",
    "print(f\"Number of records after removal of missing values: {len(df_cleaned)}\")\n",
    "\n",
    "# To apply the change directly to the original DataFrame 'df', use inplace=True\n",
    "df.dropna(how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c79087d-c5c2-415b-b9f7-49f2c9a26d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad66f8-824f-45dd-80e0-e91aee44ab20",
   "metadata": {},
   "source": [
    "**PROMPT: How to check if there any duplicates?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16124943-2834-4340-aa2f-95d1d263c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .duplicated() returns a boolean Series (True for duplicates)\n",
    "# .sum() counts the number of 'True' values (which are 1)\n",
    "total_duplicates = df.duplicated().sum()\n",
    "\n",
    "print(f\"Total number of duplicate rows found in the dataset: {total_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c83dc9f-e277-4b15-8863-7a6f62eff772",
   "metadata": {},
   "source": [
    "## Basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdbca72-afa9-4c93-9de2-40d0241d3e91",
   "metadata": {},
   "source": [
    "**PROMPT: How to obtain column names containing continuous and categorical values?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31393d9c-32ae-4c25-8ba9-d0d3b2ea0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns by basic dtype\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "object_cols = df.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d49ce6-3164-4f06-8d72-1623cff84191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum number of unique values a variable can have to be considered categorical\n",
    "# 20 is a common rule of thumb, but can be adjusted.\n",
    "UNIQUE_VALUE_THRESHOLD = 20\n",
    "\n",
    "# Initialize the final lists\n",
    "continuous_vars = []\n",
    "categorical_vars = list(object_cols) # All remaining object columns are assumed categorical\n",
    "\n",
    "# Loop through all numeric columns to apply the heuristic\n",
    "for col in numeric_cols:\n",
    "    # Use dropna() since missing values don't count towards the unique value count\n",
    "    if df[col].nunique() < UNIQUE_VALUE_THRESHOLD:\n",
    "        categorical_vars.append(col)\n",
    "    else:\n",
    "        continuous_vars.append(col)\n",
    "\n",
    "print(\"--- Final Classification of Variables ---\")\n",
    "print(f\"Continuous Variables ({len(continuous_vars)}): \\n{continuous_vars}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Categorical Variables ({len(categorical_vars)}): \\n{categorical_vars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d752ff1-a7e2-4a42-a189-62b4e695c33b",
   "metadata": {},
   "source": [
    "**PROMPT: How to display histograms for continuous data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e99e8-e56c-4772-acfa-810444301491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Identify Continuous Variables (Based on Cleaning Steps) ---\n",
    "# Assuming 'df' is the cleaned DataFrame where NaNs were dropped and types are correct.\n",
    "UNIQUE_VALUE_THRESHOLD = 20\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "continuous_vars = []\n",
    "for col in numeric_cols:\n",
    "    if df[col].nunique() >= UNIQUE_VALUE_THRESHOLD:\n",
    "        continuous_vars.append(col)\n",
    "\n",
    "print(f\"Continuous Variables identified for plotting: {continuous_vars}\")\n",
    "\n",
    "# --- 2. Generate Histograms ---\n",
    "if continuous_vars:\n",
    "    num_vars = len(continuous_vars)\n",
    "    # Set up a grid of subplots (e.g., 2 histograms per row)\n",
    "    n_rows = (num_vars + 1) // 2\n",
    "    n_cols = 2\n",
    "\n",
    "    # Create the figure and axes\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 5 * n_rows))\n",
    "    axes = axes.flatten() # Flattens the axes array for easy iteration\n",
    "\n",
    "    for i, col in enumerate(continuous_vars):\n",
    "        # Plot the histogram using 15 bins and a slight alpha for better viewing\n",
    "        axes[i].hist(df[col], bins=15, edgecolor='black', alpha=0.7)\n",
    "        axes[i].set_title(f'Distribution of {col}', fontsize=14)\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(len(continuous_vars), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.suptitle('Histograms of Continuous Variables', fontsize=18, y=1.02)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "    plt.savefig('continuous_histograms.png')\n",
    "    # plt.show() # Use plt.show() to display the plot interactively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c7c1d-5313-4801-b8b5-5eee9bf40635",
   "metadata": {},
   "source": [
    "**PROMPT: How to get basic statistic for continuous data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7501e8c-3bf9-4e39-9b78-80a35bc35d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Identify Continuous Variables (Based on Cleaning Steps) ---\n",
    "# Assuming 'df' is the cleaned DataFrame where NaNs were dropped and types are correct.\n",
    "UNIQUE_VALUE_THRESHOLD = 20\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "continuous_vars = []\n",
    "for col in numeric_cols:\n",
    "    if df[col].nunique() >= UNIQUE_VALUE_THRESHOLD:\n",
    "        continuous_vars.append(col)\n",
    "\n",
    "# --- 2. Calculate Basic Statistics ---\n",
    "if continuous_vars:\n",
    "    # Use .describe() on the subset of continuous columns\n",
    "    continuous_data_stats = df[continuous_vars].describe()\n",
    "\n",
    "    # Print the resulting table\n",
    "    print(\"\\n--- Basic Statistics for Continuous Variables ---\")\n",
    "    print(continuous_data_stats)\n",
    "else:\n",
    "    print(\"No continuous variables found for statistical analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bed3c1-2e3c-42b1-8c86-605834ecb22b",
   "metadata": {},
   "source": [
    "**PROMPT: How to visualize columns with categorical values?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a7528-15e5-4f0e-ad45-07dd8250c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is the cleaned DataFrame\n",
    "UNIQUE_VALUE_THRESHOLD = 20\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "categorical_vars = []\n",
    "for col in numeric_cols:\n",
    "    if df[col].nunique() < UNIQUE_VALUE_THRESHOLD:\n",
    "        categorical_vars.append(col)\n",
    "\n",
    "# In your dataset, this list likely includes columns like 'sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal', and 'target'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7e309-5162-494a-ad81-897425faa615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'categorical_vars' is the list generated above\n",
    "\n",
    "if categorical_vars:\n",
    "    num_vars = len(categorical_vars)\n",
    "    # Set up a grid of subplots (e.g., 3 plots per row)\n",
    "    n_rows = (num_vars + 1) // 3\n",
    "    n_cols = 3\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(categorical_vars):\n",
    "        # Use seaborn's countplot for a clean bar visualization\n",
    "        sns.countplot(x=col, data=df, ax=axes[i], palette='viridis')\n",
    "        axes[i].set_title(f'Frequency of {col}', fontsize=14)\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Count')\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(len(categorical_vars), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.suptitle('Count Plots of Categorical Variables', fontsize=18, y=1.02)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "    plt.show() # Display the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0dfcb-f697-4f46-8c42-9791daa8a920",
   "metadata": {},
   "source": [
    "**PROMPT: How to get basic statistics for categorical variables?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0657941-9bc4-4709-bd0d-d2baeb456333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'categorical_vars' is the list of column names identified earlier\n",
    "if categorical_vars:\n",
    "    # Use include='all' to get descriptive stats for categorical columns\n",
    "    categorical_stats = df[categorical_vars].describe(include='all')\n",
    "\n",
    "    print(\"--- Descriptive Statistics for Categorical Variables ---\")\n",
    "    print(categorical_stats)\n",
    "else:\n",
    "    print(\"No categorical variables found for statistical analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b63709-a019-4d20-92ed-78b566389663",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Value Counts (Frequency Distribution) for Each Categorical Variable ---\")\n",
    "\n",
    "for col in categorical_vars:\n",
    "    print(f\"\\n--- Column: {col} ---\")\n",
    "\n",
    "    # Get the count of each unique value\n",
    "    counts = df[col].value_counts()\n",
    "    print(counts)\n",
    "\n",
    "    # Optional: Get the percentage distribution\n",
    "    percentages = df[col].value_counts(normalize=True) * 100\n",
    "    print(\"\\nPercentage Distribution:\")\n",
    "    print(percentages.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7d5d1-2791-45c4-83ea-5c15b7710b6b",
   "metadata": {},
   "source": [
    "**PROMPT: Can I get a table showing the numbers and percentages for each categorical variable?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec87fc-20ca-4715-ac09-4327150bd4c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c42777ec-4b9d-49ce-ac06-b5f1ebac9fcc",
   "metadata": {},
   "source": [
    "**PROMPT: Can I see correlation matrix for my data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc277b-ec90-441a-866b-d72a31fa210b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "684cfcc8-692a-458a-a98e-cf04810c4a50",
   "metadata": {},
   "source": [
    "**PROMPT: I do not see \"sex\" in the list (Male, Female), how to include them?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf37cc-238f-4c70-9716-e1483a1122dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f77d5414-4b20-4c70-8d09-91e14c577076",
   "metadata": {},
   "source": [
    "## Statistics on groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13de8bfc-2046-4127-aec0-f198be15a47f",
   "metadata": {},
   "source": [
    "**PROMPT: Can you show basic statistics per \"sex\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7653c42-ca74-4374-82dd-7afe262358b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'df' is the cleaned DataFrame from previous steps\n",
    "\n",
    "# 1. Identify Continuous Variables (Heuristic)\n",
    "UNIQUE_VALUE_THRESHOLD = 20\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "continuous_vars = []\n",
    "for col in numeric_cols:\n",
    "    if df[col].nunique() >= UNIQUE_VALUE_THRESHOLD:\n",
    "        continuous_vars.append(col)\n",
    "\n",
    "# 2. Perform Grouped Descriptive Statistics\n",
    "if 'sex' in df.columns and continuous_vars:\n",
    "    # Select only the continuous columns and the grouping column ('sex')\n",
    "    grouped_stats = df.groupby('sex')[continuous_vars].describe()\n",
    "\n",
    "    print(\"\\n--- Basic Statistics Grouped by 'sex' ---\")\n",
    "    # Display the resulting multi-level indexed table\n",
    "    print(grouped_stats)\n",
    "else:\n",
    "    print(\"The 'sex' column or continuous variables are not available in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2626461d-58f3-46aa-9a06-f9b00866462b",
   "metadata": {},
   "source": [
    "**PROMPT: Can you show how many Men have \"chol\" level above the median for whole dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685f3076-60c2-4fa1-8d0c-613da060023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load and Clean the DataFrame (Full Preprocessing) ---\n",
    "try:\n",
    "    df = pd.read_csv('./Heart_disease_cleveland_new.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file './Heart_disease_cleveland_new.csv' was not found. Please ensure it is uploaded.\")\n",
    "    exit() # Exit if the file can't be loaded\n",
    "\n",
    "# a. Replace missing value placeholders (\"unknown\" and \"?\") with NaN\n",
    "df.replace([\"unknown\", \"?\"], np.nan, inplace=True)\n",
    "\n",
    "# b. Convert columns to numeric where possible (like 'chol')\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# c. Remove all records containing at least one empty value (dropna)\n",
    "df.dropna(how='any', inplace=True)\n",
    "\n",
    "# --- 2. Calculate the Overall Median for 'chol' ---\n",
    "median_chol = df['chol'].median()\n",
    "\n",
    "# --- 3. Filter and Count ---\n",
    "# Standard coding for 'sex' in this dataset is typically 1.0 for Men\n",
    "men_sex_code = 1.0\n",
    "\n",
    "# 3.1 Filter for Men (sex == 1.0)\n",
    "men_df = df[df['sex'] == men_sex_code]\n",
    "\n",
    "# 3.2 Filter for those with 'chol' > overall median\n",
    "men_high_chol = men_df[men_df['chol'] > median_chol]\n",
    "\n",
    "# 3.3 Get the final count\n",
    "count_men_high_chol = len(men_high_chol)\n",
    "\n",
    "# --- 4. Display Results ---\n",
    "print(\"--- Cholesterol Analysis ---\")\n",
    "print(f\"Overall Median Cholesterol (chol) for the cleaned dataset: {median_chol:.2f}\")\n",
    "print(f\"Number of Men (sex={men_sex_code}) with cholesterol above the overall median: {count_men_high_chol} üë®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b0cb4-6a27-40f7-aaf4-12555c69e5f8",
   "metadata": {},
   "source": [
    "**PROMPT: Show me records with Man who has \"chol\" levels above 200**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887d2a5-64b1-4e9e-ab7c-902d48be563e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ead3130-3a45-47e3-99bc-f634c745ea57",
   "metadata": {},
   "source": [
    "**PROMPT: How to save selected Men to a csv file?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b404fc-3fb3-4da0-9e29-1b9700c75f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c2cb07c-fee2-455a-9e1c-6aea48eccf16",
   "metadata": {},
   "source": [
    "**PROMPT: Can you show a plot of \"chol\" vs \"age\" regarding \"target\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc37588-18bc-4e30-b186-71354d1f0842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fdb240a-f7c3-44e6-b446-0e5963c8879e",
   "metadata": {},
   "source": [
    "## Statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1417cc20-a8a9-4808-ab95-9bba110458dd",
   "metadata": {},
   "source": [
    "**PROMPT: Can you show me how to check if there is statistical significance between men and women for each variable?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7890c8-f3da-4442-95de-9d167d5eeacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind, chi2_contingency\n",
    "\n",
    "# --- 1. Load and Clean the DataFrame (Full Preprocessing) ---\n",
    "# Assuming 'df' is the cleaned DataFrame where NaNs were dropped and types are correct.\n",
    "# (The code block above includes the necessary loading and cleaning steps.)\n",
    "\n",
    "# --- 2. Identify Variables (Continuous vs. Categorical) ---\n",
    "UNIQUE_VALUE_THRESHOLD = 20\n",
    "numeric_cols = [col for col in df.select_dtypes(include=['int64', 'float64']).columns if col != 'sex']\n",
    "\n",
    "continuous_vars = []\n",
    "categorical_vars = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if df[col].nunique() >= UNIQUE_VALUE_THRESHOLD:\n",
    "        continuous_vars.append(col)\n",
    "    else:\n",
    "        categorical_vars.append(col)\n",
    "\n",
    "# --- 3. Perform Statistical Tests ---\n",
    "results = {}\n",
    "alpha = 0.05\n",
    "group_men = df[df['sex'] == 1.0] # Assuming 1.0 is Men\n",
    "group_women = df[df['sex'] == 0.0] # Assuming 0.0 is Women\n",
    "\n",
    "# A. T-test for Continuous Variables\n",
    "for col in continuous_vars:\n",
    "    # Compares the mean of the variable in the Men group vs. the Women group\n",
    "    stat, p_value = ttest_ind(group_men[col], group_women[col], equal_var=True)\n",
    "\n",
    "    significance = \"Statistically Significant\" if p_value < alpha else \"Not Significant\"\n",
    "    results[col] = {\n",
    "        'Test': 'T-test (Means)',\n",
    "        'p-value': p_value,\n",
    "        'Significance': significance\n",
    "    }\n",
    "\n",
    "# B. Chi-Square Test of Independence for Categorical Variables\n",
    "for col in categorical_vars:\n",
    "    # --- ADD THIS CHECK ---\n",
    "    # Chi-Square requires at least 2 unique values for the column AND the 'sex' column\n",
    "    if df[col].nunique() < 2:\n",
    "        print(f\"‚ö†Ô∏è Skipping Chi-Square test for '{col}': Only {df[col].nunique()} unique value(s) remain.\")\n",
    "        results[col] = {\n",
    "            'Test': 'Chi-Square (Skipped)',\n",
    "            'p-value': np.nan,\n",
    "            'Significance': 'Not Enough Variation'\n",
    "        }\n",
    "        continue # Skip to the next column\n",
    "    # -----------------------\n",
    "    \n",
    "    # Create the contingency table\n",
    "    contingency_table = pd.crosstab(df['sex'], df[col])\n",
    "    \n",
    "    # Check for empty table (secondary check, mostly handled by the above nunique check)\n",
    "    if contingency_table.size == 0:\n",
    "        continue # Should not happen if nunique >= 2\n",
    "\n",
    "    # Perform the chi-square test\n",
    "    stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "    significance = \"Statistically Significant\" if p_value < alpha else \"Not Significant\"\n",
    "    results[col] = {\n",
    "        'Test': 'Chi-Square (Association)',\n",
    "        'p-value': p_value,\n",
    "        'Significance': significance\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd6443-390e-4253-a766-ca01dc26ecc2",
   "metadata": {},
   "source": [
    "**PROMPT: How to create two tables containing general statistics: total, men, women. Number and percentage for categorical variables, median and IQR range for continuous variables? Calculate the ‚Äúp-value‚Äù in each table. All floating point numbers should be rounded to three decimal places.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c5fa4-1281-48d1-aa8b-bd7ccdb83dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "261f65ef-4c7b-4790-acb7-ce5c3ead7e30",
   "metadata": {},
   "source": [
    "**PROMPT: How to make a \"p\" correction? Justify your choice of method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2929d-faac-4f9e-8c1f-f0aa59260ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34b32c08-5052-4f28-892f-99a862a275b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Discovering risk factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea04715f-3a7c-48ae-aa3d-797b4c2686ce",
   "metadata": {},
   "source": [
    "**PROMPT: How to discover risk factors for my \"target\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8025975b-9cfc-48bd-9836-6e32db5dead3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10a19a82-08e9-4acf-b858-0ccccd4f8c2e",
   "metadata": {},
   "source": [
    "**PROMPT: How to discover feature importance regarding \"target\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250b07d-ce62-453d-ba88-c648cc2aad90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8299afc-6ff4-42f0-b440-d4dd9ff505a5",
   "metadata": {},
   "source": [
    "## BONUS. Advanced prompt (don't expect it will produce expected outcome). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b3c483-a3f7-454a-9aba-4bbdd5570fa1",
   "metadata": {},
   "source": [
    "**PROMPT**:\n",
    "\n",
    "I have a cleaned and prepared Heart Disease Cleveland dataset (\"df\") (303 patients, 14 clinical variables). Now I need to conduct exploratory data analysis to formulate evidence-based research hypotheses for epidemiological investigation.\n",
    "\n",
    "Dataset context:\n",
    "- Cleaned dataset ready for analysis\n",
    "- Target: heart disease presence (binary)\n",
    "- 14 clinical predictors including demographics, symptoms, and test results\n",
    "- Balanced analysis needed for hypothesis generation\n",
    "\n",
    "Please create Python code that performs comprehensive exploratory analysis to generate research hypotheses:\n",
    "\n",
    "1. **Univariate Analysis by Heart Disease Status:**\n",
    "   - **Demographics:**\n",
    "     * Age distribution: histograms, box plots, statistical tests (t-test/Mann-Whitney)\n",
    "     * Sex distribution: cross-tabulation, chi-square test\n",
    "     * Calculate mean age difference between diseased/healthy\n",
    "     * Assess age-sex interaction patterns\n",
    "   \n",
    "   - **Clinical Symptoms:**\n",
    "     * Chest pain types (cp) distribution by heart disease status\n",
    "     * Exercise-induced angina (exang) prevalence comparison\n",
    "     * Statistical significance testing for symptom-disease associations\n",
    "   \n",
    "   - **Clinical Measurements:**\n",
    "     * Blood pressure (trestbps): distribution analysis, clinical cut-points\n",
    "     * Cholesterol (chol): distribution analysis, clinical categories\n",
    "     * Maximum heart rate (thalach): age-adjusted analysis\n",
    "     * Fasting blood sugar (fbs) prevalence comparison\n",
    "   \n",
    "   - **Diagnostic Tests:**\n",
    "     * Resting ECG (restecg) abnormalities by disease status\n",
    "     * ST depression (oldpeak) patterns and clinical significance\n",
    "     * ST segment slope (slope) distribution analysis\n",
    "     * Major vessels (ca) involvement patterns\n",
    "     * Thalassemia (thal) defect types by disease status\n",
    "\n",
    "2. **Bivariate Analysis and Correlation:**\n",
    "   - Create comprehensive correlation matrix for numerical variables\n",
    "   - Cross-tabulation tables for categorical variables\n",
    "   - Point-biserial correlations for categorical-numerical pairs\n",
    "   - Identify strongest associations with heart disease\n",
    "   - Clinical interpretation of correlation patterns\n",
    "\n",
    "3. **Advanced Visualization for Hypothesis Generation:**\n",
    "   - **Age-Sex stratified analysis:**\n",
    "     * Heart disease prevalence by age groups and sex\n",
    "     * Age-sex interaction heatmaps\n",
    "     * Stacked bar charts showing disease patterns\n",
    "   \n",
    "   - **Risk Factor Combination Analysis:**\n",
    "     * Multiple chest pain symptoms (cp + exang)\n",
    "     * Blood pressure and cholesterol combined effects\n",
    "     * Age with clinical measurements interactions\n",
    "   \n",
    "   - **Diagnostic Test Patterns:**\n",
    "     * ECG abnormalities with ST depression correlation\n",
    "     * Vessel involvement (ca) with other cardiac markers\n",
    "     * Thalassemia types with disease severity indicators\n",
    "\n",
    "4. **Statistical Evidence Summary for Hypothesis Formulation:**\n",
    "   - Calculate effect sizes (Cohen's d, Cramer's V, odds ratios)\n",
    "   - Identify variables with strongest associations (p<0.05, effect size >0.3)\n",
    "   - Rank variables by predictive potential\n",
    "   - Document clinical plausibility of observed associations\n",
    "\n",
    "5. **Generate Formal Research Hypotheses:**\n",
    "   Based on the exploratory analysis, formulate:\n",
    "   \n",
    "   **Primary Hypothesis:**\n",
    "   - Main research question with strongest evidence\n",
    "   - Specify direction and magnitude of expected effect\n",
    "   \n",
    "   **Secondary Hypotheses (3-4):**\n",
    "   - Additional hypotheses supported by exploratory findings\n",
    "   - Include interaction hypotheses if patterns suggest them\n",
    "   \n",
    "   **Each hypothesis should specify:**\n",
    "   - Null hypothesis (H‚ÇÄ)\n",
    "   - Alternative hypothesis (H‚ÇÅ)\n",
    "   - Expected effect direction and magnitude\n",
    "   - Clinical/biological rationale\n",
    "   - Statistical test to be used for verification\n",
    "   - Required assumptions for the test\n",
    "\n",
    "6. **Create Evidence Summary Tables:**\n",
    "   - Summary table of all associations found (variable, effect size, p-value, clinical interpretation)\n",
    "   - Risk factor ranking by strength of association\n",
    "   - Suggested analysis plan for hypothesis testing\n",
    "   - Power analysis estimates for main hypotheses\n",
    "\n",
    "Please ensure all analyses are clinically grounded and provide epidemiologically meaningful interpretations. Include detailed visualizations that support hypothesis formation and statistical evidence for each proposed hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa151015-5f97-4822-92ab-bfd2dba9cf35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "dd1e9b83-cf96-4ec9-a09c-d59e0d82ee45",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# ============================\n",
    "# EDA + HYPOTHESIS GENERATION PIPELINE FOR CLEVELAND HEART DATA\n",
    "# Assumes a cleaned pandas DataFrame `df` is already in memory with standard columns.\n",
    "# Only comments are provided; no prose outside the code.\n",
    "# ============================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from itertools import product, combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from statsmodels.stats.contingency_tables import Table2x2\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.stats.power import TTestIndPower, NormalIndPower\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "np.random.seed(42)\n",
    "\n",
    "TARGET = 'target'  # 0/1\n",
    "# Expected columns in the Cleveland dataset; adjust if your schema differs\n",
    "NUM_COLS = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']  # numerical/continuous\n",
    "CAT_COLS = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']  # categorical/discrete\n",
    "PLOTS_DIR = 'figures'\n",
    "RESULTS_DIR = 'results'\n",
    "ALPHA = 0.05\n",
    "FDR_Q = 0.05\n",
    "\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# UTILITIES\n",
    "# ----------------------------\n",
    "def ensure_numeric(series):\n",
    "    \"\"\"Coerce to numeric for safety.\"\"\"\n",
    "    return pd.to_numeric(series, errors='coerce')\n",
    "\n",
    "def cohen_d_independent(x, y):\n",
    "    \"\"\"Cohen's d for two independent samples (Welch-compatible pooled SD).\"\"\"\n",
    "    x = pd.Series(x).dropna()\n",
    "    y = pd.Series(y).dropna()\n",
    "    nx, ny = len(x), len(y)\n",
    "    if nx < 2 or ny < 2:\n",
    "        return np.nan\n",
    "    sx, sy = x.std(ddof=1), y.std(ddof=1)\n",
    "    # Pooled SD (unbiased); using weighted by df\n",
    "    sp = np.sqrt(((nx - 1) * sx**2 + (ny - 1) * sy**2) / (nx + ny - 2)) if (nx + ny - 2) > 0 else np.nan\n",
    "    if sp == 0 or np.isnan(sp):\n",
    "        return np.nan\n",
    "    return (x.mean() - y.mean()) / sp\n",
    "\n",
    "def rank_biserial_from_u(u_stat, n1, n2):\n",
    "    \"\"\"Rank-biserial correlation from Mann-Whitney U (ranges -1..1).\"\"\"\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return np.nan\n",
    "    return 1 - 2*u_stat/(n1*n2)\n",
    "\n",
    "def cramers_v_from_table(table):\n",
    "    \"\"\"Cramer's V for r x c contingency.\"\"\"\n",
    "    chi2, p, dof, _ = stats.chi2_contingency(table, correction=False)\n",
    "    n = table.values.sum()\n",
    "    r, c = table.shape\n",
    "    v = np.sqrt(chi2 / (n * (min(r, c) - 1))) if min(r, c) > 1 else np.nan\n",
    "    return v, chi2, p, dof\n",
    "\n",
    "def odds_ratio_2x2(table_2x2):\n",
    "    \"\"\"Odds ratio (Fisher exact) for 2x2; Haldane-Anscombe correction if zero cell.\"\"\"\n",
    "    arr = table_2x2.to_numpy().astype(float)\n",
    "    if arr.shape != (2, 2):\n",
    "        return np.nan, np.nan\n",
    "    if (arr == 0).any():\n",
    "        arr = arr + 0.5\n",
    "    or_val = (arr[0,0] * arr[1,1]) / (arr[0,1] * arr[1,0])\n",
    "    _, p = stats.fisher_exact(arr)\n",
    "    return or_val, p\n",
    "\n",
    "def normality_ok(x, alpha=0.05):\n",
    "    \"\"\"Shapiro-Wilk for n between 3 and 5000; returns True if normal.\"\"\"\n",
    "    x = pd.Series(x).dropna()\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return False\n",
    "    _, p = stats.shapiro(x)\n",
    "    return p > alpha\n",
    "\n",
    "def binarize_if_binary(series):\n",
    "    \"\"\"Return boolean mask if the series is binary (two unique values).\"\"\"\n",
    "    vals = pd.Series(series).dropna().unique()\n",
    "    if len(vals) == 2:\n",
    "        mapping = {vals[0]: 0, vals[1]: 1}\n",
    "        return pd.Series(series).map(mapping)\n",
    "    return None\n",
    "\n",
    "def safe_proportions(table_2x2):\n",
    "    \"\"\"Return p1, p2 (proportions) from 2x2 table w.r.t. column=target=1 if columns coded as 0/1.\"\"\"\n",
    "    # Expect rows = exposure (0/1), columns = outcome (0/1)\n",
    "    arr = table_2x2.to_numpy()\n",
    "    if arr.shape != (2, 2):\n",
    "        return np.nan, np.nan\n",
    "    p1 = arr[1,1] / arr[1].sum() if arr[1].sum() > 0 else np.nan\n",
    "    p0 = arr[0,1] / arr[0].sum() if arr[0].sum() > 0 else np.nan\n",
    "    return p1, p0\n",
    "\n",
    "def cut_age_groups(s):\n",
    "    \"\"\"Create clinically sensible age groups.\"\"\"\n",
    "    return pd.cut(s, bins=[-np.inf, 39, 49, 59, 69, np.inf],\n",
    "                  labels=['<40', '40-49', '50-59', '60-69', '70+'])\n",
    "\n",
    "def clinical_bp_cut(s):\n",
    "    \"\"\"Hypertension cutpoint per resting SBP >=140 mmHg.\"\"\"\n",
    "    return pd.Series(np.where(s >= 140, 'HighBP(‚â•140)', 'NormalBP(<140)'), index=s.index)\n",
    "\n",
    "def clinical_chol_cut(s):\n",
    "    \"\"\"Total cholesterol clinical categories.\"\"\"\n",
    "    cats = pd.cut(s, bins=[-np.inf, 199, 239, np.inf],\n",
    "                  labels=['Desirable(<200)', 'Borderline(200-239)', 'High(‚â•240)'])\n",
    "    return cats\n",
    "\n",
    "def savefig(path):\n",
    "    \"\"\"Save figure with tight layout.\"\"\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=120)\n",
    "    plt.close()\n",
    "\n",
    "# ----------------------------\n",
    "# 0) BASIC CHECKS\n",
    "# ----------------------------\n",
    "# Ensure required columns exist; will raise if not found\n",
    "missing_needed = [c for c in [TARGET] + NUM_COLS + CAT_COLS if c not in df.columns]\n",
    "if missing_needed:\n",
    "    raise KeyError(f\"Missing expected columns: {missing_needed}\")\n",
    "\n",
    "# Coerce numerics\n",
    "for c in NUM_COLS + [TARGET]:\n",
    "    df[c] = ensure_numeric(df[c])\n",
    "\n",
    "# Ensure categorical dtype\n",
    "for c in CAT_COLS:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype('category')\n",
    "\n",
    "# ----------------------------\n",
    "# 1) UNIVARIATE ANALYSIS BY HEART DISEASE STATUS\n",
    "# ----------------------------\n",
    "\n",
    "# (a) Demographics: Age distribution, boxplots, tests\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "# Histogram by target\n",
    "for t in sorted(df[TARGET].dropna().unique()):\n",
    "    ax1.hist(df.loc[df[TARGET] == t, 'age'].dropna(), alpha=0.5, bins=12, label=f\"target={int(t)}\")\n",
    "ax1.set_title('Age distribution by heart disease status')\n",
    "ax1.set_xlabel('Age [years]')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "# Boxplot by target\n",
    "box_data = [df.loc[df[TARGET] == t, 'age'].dropna().values for t in sorted(df[TARGET].dropna().unique())]\n",
    "ax2.boxplot(box_data, labels=[f\"t={int(t)}\" for t in sorted(df[TARGET].dropna().unique())])\n",
    "ax2.set_title('Age by heart disease status')\n",
    "ax2.set_ylabel('Age [years]')\n",
    "\n",
    "savefig(f\"{PLOTS_DIR}/age_by_target.png\")\n",
    "\n",
    "# Statistical tests for age\n",
    "age0 = df.loc[df[TARGET] == 0, 'age'].dropna()\n",
    "age1 = df.loc[df[TARGET] == 1, 'age'].dropna()\n",
    "if normality_ok(age0) and normality_ok(age1):\n",
    "    t_stat, p_age = stats.ttest_ind(age1, age0, equal_var=False)\n",
    "    test_age = 'Welch t-test'\n",
    "    eff_age = cohen_d_independent(age1, age0)\n",
    "    eff_name_age = 'Cohen_d'\n",
    "else:\n",
    "    u_stat, p_age = stats.mannwhitneyu(age1, age0, alternative='two-sided')\n",
    "    test_age = 'Mann-Whitney U'\n",
    "    eff_age = rank_biserial_from_u(u_stat, len(age1), len(age0))\n",
    "    eff_name_age = 'RankBiserial'\n",
    "mean_diff_age = age1.mean() - age0.mean()\n",
    "\n",
    "# (b) Sex distribution: cross-tab + chi-square\n",
    "ct_sex = pd.crosstab(df['sex'], df[TARGET])\n",
    "v_sex, chi2_sex, p_sex, dof_sex = cramers_v_from_table(ct_sex)\n",
    "\n",
    "# (c) Age-Sex interaction (two-way prevalence table)\n",
    "age_groups = cut_age_groups(df['age'])\n",
    "prev_age_sex = pd.crosstab([age_groups, df['sex']], df[TARGET], normalize='index').rename(columns={0:'Prev_noHD',1:'Prev_HD'})\n",
    "prev_age_sex.to_csv(f\"{RESULTS_DIR}/prev_age_sex.csv\", index=True)\n",
    "\n",
    "# (d) Clinical symptoms: cp distribution, exang prevalence, tests\n",
    "ct_cp = pd.crosstab(df['cp'], df[TARGET])\n",
    "v_cp, chi2_cp, p_cp, dof_cp = cramers_v_from_table(ct_cp)\n",
    "\n",
    "ct_exang = pd.crosstab(df['exang'], df[TARGET])\n",
    "v_exang, chi2_exang, p_exang, dof_exang = cramers_v_from_table(ct_exang)\n",
    "\n",
    "# (e) Clinical measurements: trestbps, chol, thalach, fbs\n",
    "# trestbps tests\n",
    "bp0 = df.loc[df[TARGET] == 0, 'trestbps'].dropna()\n",
    "bp1 = df.loc[df[TARGET] == 1, 'trestbps'].dropna()\n",
    "if normality_ok(bp0) and normality_ok(bp1):\n",
    "    t_bp, p_bp = stats.ttest_ind(bp1, bp0, equal_var=False)\n",
    "    test_bp = 'Welch t-test'\n",
    "    eff_bp = cohen_d_independent(bp1, bp0)\n",
    "    eff_name_bp = 'Cohen_d'\n",
    "else:\n",
    "    u_bp, p_bp = stats.mannwhitneyu(bp1, bp0, alternative='two-sided')\n",
    "    test_bp = 'Mann-Whitney U'\n",
    "    eff_bp = rank_biserial_from_u(u_bp, len(bp1), len(bp0))\n",
    "    eff_name_bp = 'RankBiserial'\n",
    "\n",
    "# chol tests\n",
    "ch0 = df.loc[df[TARGET] == 0, 'chol'].dropna()\n",
    "ch1 = df.loc[df[TARGET] == 1, 'chol'].dropna()\n",
    "if normality_ok(ch0) and normality_ok(ch1):\n",
    "    t_ch, p_ch = stats.ttest_ind(ch1, ch0, equal_var=False)\n",
    "    test_ch = 'Welch t-test'\n",
    "    eff_ch = cohen_d_independent(ch1, ch0)\n",
    "    eff_name_ch = 'Cohen_d'\n",
    "else:\n",
    "    u_ch, p_ch = stats.mannwhitneyu(ch1, ch0, alternative='two-sided')\n",
    "    test_ch = 'Mann-Whitney U'\n",
    "    eff_ch = rank_biserial_from_u(u_ch, len(ch1), len(ch0))\n",
    "    eff_name_ch = 'RankBiserial'\n",
    "\n",
    "# thalach tests (age-adjusted visualization later)\n",
    "th0 = df.loc[df[TARGET] == 0, 'thalach'].dropna()\n",
    "th1 = df.loc[df[TARGET] == 1, 'thalach'].dropna()\n",
    "if normality_ok(th0) and normality_ok(th1):\n",
    "    t_th, p_th = stats.ttest_ind(th1, th0, equal_var=False)\n",
    "    test_th = 'Welch t-test'\n",
    "    eff_th = cohen_d_independent(th1, th0)\n",
    "    eff_name_th = 'Cohen_d'\n",
    "else:\n",
    "    u_th, p_th = stats.mannwhitneyu(th1, th0, alternative='two-sided')\n",
    "    test_th = 'Mann-Whitney U'\n",
    "    eff_th = rank_biserial_from_u(u_th, len(th1), len(th0))\n",
    "    eff_name_th = 'RankBiserial'\n",
    "\n",
    "# fbs prevalence comparison\n",
    "ct_fbs = pd.crosstab(df['fbs'], df[TARGET])\n",
    "v_fbs, chi2_fbs, p_fbs, dof_fbs = cramers_v_from_table(ct_fbs)\n",
    "\n",
    "# (f) Diagnostic tests: restecg, oldpeak, slope, ca, thal\n",
    "ct_rest = pd.crosstab(df['restecg'], df[TARGET]); v_rest, chi2_rest, p_rest, dof_rest = cramers_v_from_table(ct_rest)\n",
    "op0 = df.loc[df[TARGET]==0, 'oldpeak'].dropna(); op1 = df.loc[df[TARGET]==1, 'oldpeak'].dropna()\n",
    "if normality_ok(op0) and normality_ok(op1):\n",
    "    t_op, p_op = stats.ttest_ind(op1, op0, equal_var=False); test_op='Welch t-test'; eff_op=cohen_d_independent(op1,op0); eff_name_op='Cohen_d'\n",
    "else:\n",
    "    u_op, p_op = stats.mannwhitneyu(op1, op0, alternative='two-sided'); test_op='Mann-Whitney U'; eff_op=rank_biserial_from_u(u_op,len(op1),len(op0)); eff_name_op='RankBiserial'\n",
    "ct_slope = pd.crosstab(df['slope'], df[TARGET]); v_slope, chi2_slope, p_slope, dof_slope = cramers_v_from_table(ct_slope)\n",
    "ct_ca = pd.crosstab(df['ca'], df[TARGET]); v_ca, chi2_ca, p_ca, dof_ca = cramers_v_from_table(ct_ca)\n",
    "ct_thal = pd.crosstab(df['thal'], df[TARGET]); v_thal, chi2_thal, p_thal, dof_thal = cramers_v_from_table(ct_thal)\n",
    "\n",
    "# Visuals: clinical cut-points for BP and Chol\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "bp_cat = clinical_bp_cut(df['trestbps'])\n",
    "prev_bp = pd.crosstab(bp_cat, df[TARGET], normalize='index')\n",
    "prev_bp.plot(kind='bar', ax=ax, legend=True, rot=0, title='Heart disease prevalence by BP category')\n",
    "ax.set_ylabel('Proportion')\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "chol_cat = clinical_chol_cut(df['chol'])\n",
    "prev_chol = pd.crosstab(chol_cat, df[TARGET], normalize='index')\n",
    "prev_chol.plot(kind='bar', ax=ax2, legend=True, rot=0, title='Heart disease prevalence by Chol category')\n",
    "ax2.set_ylabel('Proportion')\n",
    "\n",
    "savefig(f\"{PLOTS_DIR}/prevalence_bp_chol.png\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) BIVARIATE + CORRELATION\n",
    "# ----------------------------\n",
    "# Numeric-numeric correlation (Pearson + Spearman)\n",
    "num_df = df[NUM_COLS].apply(pd.to_numeric, errors='coerce')\n",
    "corr_pearson = num_df.corr(method='pearson')\n",
    "corr_spearman = num_df.corr(method='spearman')\n",
    "corr_pearson.to_csv(f\"{RESULTS_DIR}/corr_numeric_pearson.csv\")\n",
    "corr_spearman.to_csv(f\"{RESULTS_DIR}/corr_numeric_spearman.csv\")\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(corr_pearson, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(NUM_COLS)), NUM_COLS, rotation=45, ha='right')\n",
    "plt.yticks(range(len(NUM_COLS)), NUM_COLS)\n",
    "plt.title(\"Pearson correlation (numeric)\")\n",
    "savefig(f\"{PLOTS_DIR}/corr_numeric_pearson.png\")\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(corr_spearman, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(NUM_COLS)), NUM_COLS, rotation=45, ha='right')\n",
    "plt.yticks(range(len(NUM_COLS)), NUM_COLS)\n",
    "plt.title(\"Spearman correlation (numeric)\")\n",
    "savefig(f\"{PLOTS_DIR}/corr_numeric_spearman.png\")\n",
    "\n",
    "# Point-biserial correlations: target (binary) vs each numeric\n",
    "pb_results = []\n",
    "for c in NUM_COLS:\n",
    "    x = df[c]\n",
    "    y = df[TARGET]\n",
    "    mask = x.notna() & y.notna()\n",
    "    if mask.sum() >= 3:\n",
    "        r_pb, p_pb = stats.pointbiserialr(y[mask], x[mask])\n",
    "        pb_results.append({'var': c, 'pointbiserial_r': r_pb, 'p_value': p_pb, 'n': int(mask.sum())})\n",
    "pb_df = pd.DataFrame(pb_results).sort_values('p_value')\n",
    "pb_df.to_csv(f\"{RESULTS_DIR}/pointbiserial_target_numeric.csv\", index=False)\n",
    "\n",
    "# Cross-tab for categorical vs target\n",
    "xtabs = {}\n",
    "for c in CAT_COLS:\n",
    "    xtabs[c] = pd.crosstab(df[c], df[TARGET], margins=False)\n",
    "    xtabs[c].to_csv(f\"{RESULTS_DIR}/crosstab_{c}_vs_target.csv\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) ADVANCED VISUALIZATION FOR HYPOTHESIS GENERATION\n",
    "# ----------------------------\n",
    "\n",
    "# (a) Age-Sex stratified prevalence and heatmap\n",
    "age_cat = cut_age_groups(df['age'])\n",
    "prev_table = pd.crosstab([age_cat, df['sex']], df[TARGET], normalize='index').rename(columns={1:'Prev_HD'}).fillna(0)\n",
    "prev_pivot = prev_table['Prev_HD'].unstack('sex').fillna(0)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.imshow(prev_pivot, aspect='auto')\n",
    "plt.colorbar(label='Prevalence of heart disease')\n",
    "plt.xticks(range(prev_pivot.shape[1]), list(prev_pivot.columns))\n",
    "plt.yticks(range(prev_pivot.shape[0]), list(prev_pivot.index))\n",
    "plt.title('Heart disease prevalence by Age group and Sex')\n",
    "savefig(f\"{PLOTS_DIR}/prevalence_heatmap_age_sex.png\")\n",
    "prev_pivot.to_csv(f\"{RESULTS_DIR}/prevalence_by_age_sex.csv\")\n",
    "\n",
    "# Stacked bar by age groups and sex\n",
    "stack_df = pd.crosstab([age_cat, df['sex']], df[TARGET], normalize='index')\n",
    "stack_df.plot(kind='bar', stacked=True, figsize=(9,4), title='Stacked prevalence by Age group & Sex')\n",
    "plt.ylabel('Proportion')\n",
    "savefig(f\"{PLOTS_DIR}/stacked_prevalence_age_sex.png\")\n",
    "\n",
    "# (b) Risk factor combinations: cp + exang; BP + Chol; Age x Thalach\n",
    "# cp + exang\n",
    "comb_cp_ex = pd.crosstab(index=[df['cp'].astype(str), df['exang'].astype(str)], columns=df[TARGET], normalize='index')\n",
    "comb_cp_ex.to_csv(f\"{RESULTS_DIR}/comb_cp_exang_vs_target.csv\")\n",
    "comb_cp_ex.plot(kind='bar', stacked=True, figsize=(10,4), title='Prevalence by cp + exang combinations')\n",
    "plt.ylabel('Proportion')\n",
    "savefig(f\"{PLOTS_DIR}/comb_cp_exang.png\")\n",
    "\n",
    "# BP + Chol (clinical categories)\n",
    "bp_cat = clinical_bp_cut(df['trestbps'])\n",
    "chol_cat = clinical_chol_cut(df['chol'])\n",
    "comb_bp_chol = pd.crosstab(index=[bp_cat, chol_cat], columns=df[TARGET], normalize='index')\n",
    "comb_bp_chol.to_csv(f\"{RESULTS_DIR}/comb_bp_chol_vs_target.csv\")\n",
    "comb_bp_chol.plot(kind='bar', stacked=True, figsize=(10,4), title='Prevalence by BP + Chol categories')\n",
    "plt.ylabel('Proportion')\n",
    "savefig(f\"{PLOTS_DIR}/comb_bp_chol.png\")\n",
    "\n",
    "# Age group x thalach (low/high by median)\n",
    "thalach_cut = pd.Series(np.where(df['thalach'] >= df['thalach'].median(), 'HighThalach', 'LowThalach'))\n",
    "comb_age_thalach = pd.crosstab(index=[age_cat, thalach_cut], columns=df[TARGET], normalize='index')\n",
    "comb_age_thalach.to_csv(f\"{RESULTS_DIR}/comb_age_thalach_vs_target.csv\")\n",
    "comb_age_thalach.plot(kind='bar', stacked=True, figsize=(10,4), title='Prevalence by Age group x Thalach')\n",
    "plt.ylabel('Proportion')\n",
    "savefig(f\"{PLOTS_DIR}/comb_age_thalach.png\")\n",
    "\n",
    "# (c) Diagnostic test patterns\n",
    "# ECG abnormalities vs oldpeak (scatter/jitter)\n",
    "plt.figure(figsize=(7,4))\n",
    "x_ecg = df['restecg'].cat.codes\n",
    "y_oldpeak = df['oldpeak']\n",
    "plt.scatter(x_ecg + (np.random.rand(len(x_ecg))-0.5)*0.15, y_oldpeak, alpha=0.6)\n",
    "plt.xticks(sorted(x_ecg.unique()), list(df['restecg'].cat.categories))\n",
    "plt.xlabel('restecg category')\n",
    "plt.ylabel('oldpeak (ST depression)')\n",
    "plt.title('oldpeak by restecg category (jittered)')\n",
    "savefig(f\"{PLOTS_DIR}/oldpeak_by_restecg_scatter.png\")\n",
    "\n",
    "# Vessel involvement (ca) vs prevalence\n",
    "prev_ca = pd.crosstab(df['ca'], df[TARGET], normalize='index')\n",
    "prev_ca.plot(kind='bar', stacked=True, rot=0, figsize=(6,4), title='Prevalence by number of major vessels (ca)')\n",
    "plt.ylabel('Proportion')\n",
    "savefig(f\"{PLOTS_DIR}/prevalence_by_ca.png\")\n",
    "\n",
    "# Thal types vs oldpeak distribution\n",
    "plt.figure(figsize=(7,4))\n",
    "data = [df.loc[df['thal'] == k, 'oldpeak'].dropna().values for k in df['thal'].cat.categories]\n",
    "plt.boxplot(data, labels=list(df['thal'].cat.categories))\n",
    "plt.ylabel('oldpeak')\n",
    "plt.title('oldpeak by thal category')\n",
    "savefig(f\"{PLOTS_DIR}/oldpeak_by_thal_box.png\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) STATISTICAL EVIDENCE SUMMARY (EFFECT SIZES, FDR, RANKING)\n",
    "# ----------------------------\n",
    "results = []\n",
    "\n",
    "# Continuous vs target loop\n",
    "for c in NUM_COLS:\n",
    "    g0 = df.loc[df[TARGET]==0, c].dropna()\n",
    "    g1 = df.loc[df[TARGET]==1, c].dropna()\n",
    "    if len(g0) >= 3 and len(g1) >= 3:\n",
    "        if normality_ok(g0) and normality_ok(g1):\n",
    "            stat, p = stats.ttest_ind(g1, g0, equal_var=False)\n",
    "            test = 'Welch t-test'\n",
    "            eff = cohen_d_independent(g1, g0)\n",
    "            eff_name = 'Cohen_d'\n",
    "            direction = 'higher_in_HD' if g1.mean() > g0.mean() else 'lower_in_HD'\n",
    "        else:\n",
    "            u, p = stats.mannwhitneyu(g1, g0, alternative='two-sided')\n",
    "            test = 'Mann-Whitney U'\n",
    "            eff = rank_biserial_from_u(u, len(g1), len(g0))\n",
    "            eff_name = 'RankBiserial'\n",
    "            direction = 'higher_in_HD' if np.median(g1) > np.median(g0) else 'lower_in_HD'\n",
    "        results.append({'variable': c, 'type': 'numeric', 'test': test, 'stat': stat if test.startswith('Welch') else u,\n",
    "                        'p_value': p, 'effect_name': eff_name, 'effect_size': eff, 'direction': direction,\n",
    "                        'n0': len(g0), 'n1': len(g1)})\n",
    "\n",
    "# Categorical vs target loop\n",
    "for c in CAT_COLS:\n",
    "    tab = pd.crosstab(df[c], df[TARGET])\n",
    "    if tab.values.sum() == 0 or tab.shape[0] < 1 or tab.shape[1] < 1:\n",
    "        continue\n",
    "    v, chi2, p, dof = cramers_v_from_table(tab)\n",
    "    results.append({'variable': c, 'type': 'categorical', 'test': 'Chi-square', 'stat': chi2,\n",
    "                    'p_value': p, 'effect_name': 'Cramers_V', 'effect_size': v, 'direction': 'n/a',\n",
    "                    'n0': int(tab[0].sum()) if 0 in tab.columns else np.nan,\n",
    "                    'n1': int(tab[1].sum()) if 1 in tab.columns else np.nan})\n",
    "    # Add odds ratio for binary 2x2 only\n",
    "    if tab.shape == (2,2):\n",
    "        or_val, p_or = odds_ratio_2x2(tab)\n",
    "        results.append({'variable': f'{c} (OR 2x2)', 'type': 'categorical', 'test': 'Fisher exact',\n",
    "                        'stat': np.nan, 'p_value': p_or, 'effect_name': 'OddsRatio', 'effect_size': or_val,\n",
    "                        'direction': 'OR>1 favors HD', 'n0': int(tab[0].sum()), 'n1': int(tab[1].sum())})\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "# FDR within numeric and within categorical families (optional); here global FDR\n",
    "res_df['p_adj_BH'] = multipletests(res_df['p_value'], method='fdr_bh')[1]\n",
    "res_df['sig_FDR'] = res_df['p_adj_BH'] < FDR_Q\n",
    "res_df.to_csv(f\"{RESULTS_DIR}/evidence_summary_all.csv\", index=False)\n",
    "\n",
    "# Rank by predictive potential: significant & |effect|>0.3\n",
    "rank_df = (res_df\n",
    "           .assign(abs_eff=lambda d: d['effect_size'].abs())\n",
    "           .sort_values(['sig_FDR','abs_eff','p_adj_BH'], ascending=[False, False, True]))\n",
    "rank_df.to_csv(f\"{RESULTS_DIR}/ranked_variables.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) GENERATE FORMAL RESEARCH HYPOTHESES (PROGRAMMATIC TEMPLATES)\n",
    "# ----------------------------\n",
    "# Select top candidates from numeric (by |effect|) and categorical (by Cramer's V / OR)\n",
    "top_numeric = (res_df[(res_df['type']=='numeric') & res_df['sig_FDR']]\n",
    "               .sort_values('effect_size', key=np.abs, ascending=False).head(3))\n",
    "top_categorical = (res_df[(res_df['type']=='categorical') & res_df['sig_FDR'] & (res_df['effect_name']=='Cramers_V')]\n",
    "                   .sort_values('effect_size', ascending=False).head(3))\n",
    "\n",
    "hypotheses = []\n",
    "\n",
    "# Primary hypothesis: choose strongest signal overall\n",
    "if not rank_df.empty:\n",
    "    primary = rank_df.iloc[0]\n",
    "    var = primary['variable']\n",
    "    eff = primary['effect_size']\n",
    "    p = primary['p_adj_BH']\n",
    "    if primary['type'] == 'numeric':\n",
    "        direction = 'increases' if eff > 0 else 'decreases'\n",
    "        test_name = 'Welch t-test or Mann-Whitney (as appropriate); logistic regression for confirmation'\n",
    "        effect_desc = f\"expected standardized effect ‚âà {eff:.2f}\"\n",
    "    else:\n",
    "        direction = 'associated with'\n",
    "        test_name = 'Chi-square (or Fisher for sparse), logistic regression'\n",
    "        effect_desc = f\"expected association strength (Cramer\\'s V) ‚âà {eff:.2f}\"\n",
    "    hypotheses.append({\n",
    "        'title': f\"Primary: {var} {direction} heart disease risk\",\n",
    "        'H0': f\"No association between {var} and heart disease.\",\n",
    "        'H1': f\"{var} is {direction} heart disease risk; {effect_desc}.\",\n",
    "        'test': test_name,\n",
    "        'alpha': ALPHA,\n",
    "        'FDR_q': FDR_Q,\n",
    "        'note': f\"Selected by strongest effect (FDR-adjusted p={p:.3g}).\"\n",
    "    })\n",
    "\n",
    "# Secondary hypotheses from remaining top signals\n",
    "for row in pd.concat([top_numeric, top_categorical]).head(4).itertuples():\n",
    "    var = row.variable\n",
    "    eff = row.effect_size\n",
    "    if row.type == 'numeric':\n",
    "        direction = 'higher in diseased' if row.direction=='higher_in_HD' else 'lower in diseased'\n",
    "        test_name = 'Welch t-test or Mann-Whitney; confirm via logistic regression with age/sex adjustment'\n",
    "        effect_desc = f\"expected effect size ‚âà {eff:.2f}\"\n",
    "    else:\n",
    "        direction = 'associated with disease status'\n",
    "        test_name = 'Chi-square/Fisher; confirm via logistic regression'\n",
    "        effect_desc = f\"expected Cramer\\'s V ‚âà {eff:.2f}\"\n",
    "    hypotheses.append({\n",
    "        'title': f\"Secondary: {var} {direction}\",\n",
    "        'H0': f\"No association between {var} and disease status.\",\n",
    "        'H1': f\"{var} is {direction}; {effect_desc}.\",\n",
    "        'test': test_name,\n",
    "        'alpha': ALPHA,\n",
    "        'FDR_q': FDR_Q,\n",
    "        'note': \"Derived from EDA signals.\"\n",
    "    })\n",
    "\n",
    "# vals: matrix of prevalence values (float), handle possible NaNs safely\n",
    "vals = np.asarray(prev_pivot.values, dtype=float)\n",
    "\n",
    "# range across cells (nan-safe); triggers interaction hypothesis if > 0.15\n",
    "rng = np.nanmax(vals) - np.nanmin(vals) if vals.size else 0.0\n",
    "\n",
    "if vals.size and np.isfinite(rng) and rng > 0.15:\n",
    "    hypotheses.append({\n",
    "        'title': \"Secondary (Interaction): Age √ó Sex modifies heart disease prevalence\",\n",
    "        'H0': \"No interaction between age group and sex on heart disease prevalence.\",\n",
    "        'H1': \"Prevalence increases with age more steeply in one sex (interaction present).\",\n",
    "        'test': \"Logistic regression with AgeGroup, Sex, and AgeGroup√óSex term; LRT for interaction\",\n",
    "        'alpha': ALPHA,\n",
    "        'FDR_q': FDR_Q,\n",
    "        'note': \"Triggered by heatmap heterogeneity.\"\n",
    "    })\n",
    "\n",
    "# Export hypotheses as a human-readable CSV/text\n",
    "hypo_df = pd.DataFrame(hypotheses)\n",
    "hypo_df.to_csv(f\"{RESULTS_DIR}/hypotheses_generated.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) EVIDENCE SUMMARY TABLES + POWER ANALYSIS\n",
    "# ----------------------------\n",
    "# Assemble a compact summary table with clinical-style interpretation placeholders\n",
    "summary_rows = []\n",
    "for row in res_df.itertuples():\n",
    "    var = row.variable\n",
    "    eff = row.effect_size\n",
    "    p_adj = row.p_adj_BH\n",
    "    interpretable = ''\n",
    "    if row.type == 'numeric' and not np.isnan(eff):\n",
    "        interpretable = f\"{row.effect_name}={eff:.2f} ({row.direction})\"\n",
    "    elif row.effect_name == 'Cramers_V' and not np.isnan(eff):\n",
    "        interpretable = f\"Cram√©r's V={eff:.2f} (association)\"\n",
    "    elif row.effect_name == 'OddsRatio' and not np.isnan(eff):\n",
    "        interpretable = f\"OR‚âà{eff:.2f} (>1 suggests higher odds in exposed)\"\n",
    "    summary_rows.append({\n",
    "        'variable': var,\n",
    "        'type': row.type,\n",
    "        'test_used': row.test,\n",
    "        'stat_or_effect': interpretable,\n",
    "        'p_value': row.p_value,\n",
    "        'p_adj_BH': p_adj,\n",
    "        'significant_FDR': row.sig_FDR\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(['significant_FDR','p_adj_BH'], ascending=[False, True])\n",
    "summary_df.to_csv(f\"{RESULTS_DIR}/evidence_summary_clinical.csv\", index=False)\n",
    "\n",
    "# Rank by strength (significant and |effect|>0.3)\n",
    "rank_filtered = (res_df.assign(abs_eff=lambda d: d['effect_size'].abs())\n",
    "                 .query('sig_FDR and abs_eff > 0.3')\n",
    "                 .sort_values(['abs_eff','p_adj_BH'], ascending=[False, True]))\n",
    "rank_filtered[['variable','effect_name','effect_size','p_adj_BH']].to_csv(f\"{RESULTS_DIR}/risk_factor_ranking.csv\", index=False)\n",
    "\n",
    "# Power analysis for primary numeric and 2x2 hypotheses (rough, based on observed effect sizes)\n",
    "power_results = []\n",
    "\n",
    "# Numeric (two-sample t-test) example: use strongest numeric signal\n",
    "num_sig = res_df[(res_df['type']=='numeric') & res_df['sig_FDR']].copy()\n",
    "if not num_sig.empty:\n",
    "    best = num_sig.iloc[num_sig['effect_size'].abs().argmax()]\n",
    "    d = np.nan_to_num(best['effect_size'])\n",
    "    # Estimate group sizes from dataset\n",
    "    n0 = int((df[TARGET]==0).sum())\n",
    "    n1 = int((df[TARGET]==1).sum())\n",
    "    ratio = n1 / max(n0,1)\n",
    "    tpower = TTestIndPower()\n",
    "    # Required per-group n for 80% power\n",
    "    try:\n",
    "        n_per_group = math.ceil(tpower.solve_power(effect_size=abs(d), alpha=ALPHA, power=0.80, ratio=ratio))\n",
    "    except Exception:\n",
    "        n_per_group = np.nan\n",
    "    # Achieved power with current sample\n",
    "    try:\n",
    "        achieved = tpower.power(effect_size=abs(d), alpha=ALPHA, nobs1=n0, ratio=ratio)\n",
    "    except Exception:\n",
    "        achieved = np.nan\n",
    "    power_results.append({'hypothesis': f'Numeric: {best.variable}', 'effect': d, 'required_n_per_group_80pct': n_per_group, 'achieved_power': achieved})\n",
    "\n",
    "# Binary 2x2 example: pick a binary categorical with strongest OR signal if available\n",
    "or_rows = res_df[res_df['effect_name']=='OddsRatio'].copy()\n",
    "if not or_rows.empty:\n",
    "    or_best = or_rows.iloc[or_rows['p_adj_BH'].argmin()]\n",
    "    # Reconstruct 2x2 and compute p1, p0\n",
    "    var = or_best['variable'].replace(' (OR 2x2)','')\n",
    "    tab = pd.crosstab(df[var], df[TARGET])\n",
    "    if tab.shape == (2,2):\n",
    "        p1, p0 = safe_proportions(tab)\n",
    "        if np.isfinite(p1) and np.isfinite(p0):\n",
    "            es = proportion_effectsize(p1, p0)  # Cohen's h\n",
    "            npower = NormalIndPower()\n",
    "            try:\n",
    "                n_per_group = math.ceil(npower.solve_power(effect_size=abs(es), alpha=ALPHA, power=0.80, ratio=1.0))\n",
    "            except Exception:\n",
    "                n_per_group = np.nan\n",
    "            try:\n",
    "                achieved = npower.power(effect_size=abs(es), alpha=ALPHA, nobs1=int(tab.iloc[0].sum()), ratio=1.0)\n",
    "            except Exception:\n",
    "                achieved = np.nan\n",
    "            power_results.append({'hypothesis': f'Binary: {var}', 'effect_Cohen_h': es, 'required_n_per_group_80pct': n_per_group, 'achieved_power': achieved})\n",
    "\n",
    "pd.DataFrame(power_results).to_csv(f\"{RESULTS_DIR}/power_analysis_estimates.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# PRINT KEY SUMMARIES (OPTIONAL)\n",
    "# ----------------------------\n",
    "print(\"Saved:\")\n",
    "print(f\"- Figures to: {PLOTS_DIR}/\")\n",
    "print(f\"- Numeric correlations: {RESULTS_DIR}/corr_numeric_*.csv\")\n",
    "print(f\"- Crosstabs: {RESULTS_DIR}/crosstab_*_vs_target.csv\")\n",
    "print(f\"- Evidence summary: {RESULTS_DIR}/evidence_summary_all.csv\")\n",
    "print(f\"- Ranked variables: {RESULTS_DIR}/ranked_variables.csv\")\n",
    "print(f\"- Hypotheses: {RESULTS_DIR}/hypotheses_generated.csv\")\n",
    "print(f\"- Power analysis: {RESULTS_DIR}/power_analysis_estimates.csv\")\n",
    "\n",
    "# ----------------------------\n",
    "# NOTES (in-code, non-executable):\n",
    "# - Effect-size thresholds: consider |d|‚â•0.3 (small-moderate), Cram√©r‚Äôs V‚â•0.3 as meaningful screens for hypotheses.\n",
    "# - Odds ratios computed only for binary 2x2 tables. For multi-level factors, use logistic regression to estimate per-level OR (ref vs baseline).\n",
    "# - For thalach age-adjustment beyond visuals, fit a simple logistic regression (target ~ thalach + age) as confirmatory step later.\n",
    "# - Generated hypotheses are templates filled with observed directions/effects and are meant to guide confirmatory modeling, not serve as final proof.\n",
    "# ============================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
